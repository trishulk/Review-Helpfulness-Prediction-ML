{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "import textstat\n",
    "import swifter\n",
    "\n",
    "import ast\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GET ALL THE DATAFRAMES :-\n",
    "# 1.NORMAL REVIEWS\n",
    "# 2.REVIEW TEXT Characteristics\n",
    "# 3.USER Charecteristics\n",
    "# 4.REVIEW METADATA Characteristics\n",
    "# 5.PRODUCT METADATA\n",
    "\n",
    "review_df = pd.read_csv('Amazon_Latest_Data.csv')\n",
    "text_df = pd.read_csv('Text_Parameters.csv')\n",
    "user_df = pd.read_csv('User_DF.csv')\n",
    "meta_df = pd.read_csv('Review_Meta_Data.csv')\n",
    "product_data_df = pd.read_csv('meta_data_latest.csv')\n",
    "helper_df = pd.read_csv('helper_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'reviewerName', 'reviewText', 'user_deviation',\n",
       "       'user_delay', 'helpful_votes', 'total_votes', 'no_of_reviews',\n",
       "       'all_helpful_votes', 'all_total_votes', 'all_no_of_reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'stem_sim_length', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation','user_delay','no_of_reviews']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count         wps  \\\n",
       "0              14.000000       176.92               1  427.000000   \n",
       "1               8.666667       175.64               2  423.000000   \n",
       "2               7.666667        13.01              23   19.521739   \n",
       "3               7.200000        14.00               5   12.800000   \n",
       "4               4.400000        61.55               1  138.000000   \n",
       "\n",
       "   review_length  pos_no  neg_no  user_deviation  user_delay  no_of_reviews  \\\n",
       "0            427       2       4        1.399189  10152000.0              1   \n",
       "1            846      18       8        0.624437  10886400.0              1   \n",
       "2            449       7       6        0.510375   7516800.0              1   \n",
       "3             64       2       0        0.442171   2678400.0              1   \n",
       "4            138       1       3        1.616074   7603200.0              1   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.037214149448659396\n",
      "Root Mean Squared Error (RMSE): 0.1929096924694542\n",
      "Mean Absolute Error (MAE):      0.13303612335141038\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg_2 = xgb.XGBRegressor(learning_rate=0.09, n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_2=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.037214149448659396\n",
      "Root Mean Squared Error (RMSE): 0.1929096924694542\n",
      "Mean Absolute Error (MAE):      0.13303612335141038\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions_2)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions_2))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions_2)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.01,0.09,0.1,0.2,0.5,0.9], \n",
    "    'n_estimators': [200,300,400,500], \n",
    "    'subsample': [0.3, 0.5, 0.9, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.09, 0.1, 0.2, 0.5, 0.9]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dict={'learning_rate':23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['learning_rate','n_estimators', 'subsample', 'rmse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  n_estimators  subsample  rmse  mae\n",
       "0           23.0           NaN        NaN   NaN  NaN\n",
       "1           23.0           NaN        NaN   NaN  NaN"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result_df.append(test_dict, ignore_index=True)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Grid Search -  22nd April 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20120225962697713, 'mae': 0.15557041057187085}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.2014565968018854, 'mae': 0.1557875063361773}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20168651505571994, 'mae': 0.15594161177602933}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20173112319307335, 'mae': 0.15598355410201298}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19531630693234425, 'mae': 0.1407569386813951}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1955291741974878, 'mae': 0.14093732727325198}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1957360203909724, 'mae': 0.1411491700239321}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19581710426115453, 'mae': 0.14121238169282893}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1939529724877901, 'mae': 0.13595449039390137}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19407229936065315, 'mae': 0.13607674326632288}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1942669772192065, 'mae': 0.13634867753773589}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19436642637252308, 'mae': 0.13646748554650368}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.1934651644703302, 'mae': 0.13417407777145268}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19351077881358725, 'mae': 0.1342698032419107}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19368385915603525, 'mae': 0.13457764060481842}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.193770156856822, 'mae': 0.13470093913001513}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19326885798257532, 'mae': 0.13287875324453233}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19281035261676624, 'mae': 0.13270153830986256}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.1925886261424162, 'mae': 0.13269917820051016}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19267762847141584, 'mae': 0.13279616820645831}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19326615755210677, 'mae': 0.13295800559319024}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19269858242931895, 'mae': 0.1327316545979626}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19243513983724747, 'mae': 0.13251727243448402}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1926161235749727, 'mae': 0.13268134519063862}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1933105189972598, 'mae': 0.13277446908700782}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19282465638634216, 'mae': 0.13258395921815197}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19237995453006587, 'mae': 0.13237502045293073}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19272364244627296, 'mae': 0.13268974812877782}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.1934340270323044, 'mae': 0.13268064936248497}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.1928075239448761, 'mae': 0.13242484739378171}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19241162338346154, 'mae': 0.13232202289831785}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19270394707732325, 'mae': 0.13257946104168297}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19305935693640455, 'mae': 0.13278172583450912}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19287213244778942, 'mae': 0.13272096876512482}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19267163768612278, 'mae': 0.13271460743356905}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19270317079483293, 'mae': 0.1327657233648706}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.1931103540651519, 'mae': 0.13295903233168913}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19275379797437106, 'mae': 0.13268438311818612}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19252124833429843, 'mae': 0.1325474779399158}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1926948940536101, 'mae': 0.13269775029500197}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19335741794993874, 'mae': 0.1328290462186427}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19308895774401338, 'mae': 0.13259518862561875}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19244936186099615, 'mae': 0.13239797794786146}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1926608593386005, 'mae': 0.1326025597183714}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19367796087006608, 'mae': 0.13285694933834502}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19317808936726671, 'mae': 0.13253551883780157}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.1925034388535176, 'mae': 0.13238602418662904}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19269715965110984, 'mae': 0.13254598262501208}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.1947746259270443, 'mae': 0.13329514211399254}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19351091651719027, 'mae': 0.13270986892109768}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19275810037142818, 'mae': 0.13264940712783185}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1928782268878543, 'mae': 0.13266735954202513}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19498584162682733, 'mae': 0.1338088764648932}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19362829324645922, 'mae': 0.1329523042147439}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19285377753422028, 'mae': 0.1326247269407523}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19297780173259066, 'mae': 0.13262146869841893}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1958825401021449, 'mae': 0.13413933481373044}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19436220527056297, 'mae': 0.13310469248848927}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19323129471784606, 'mae': 0.1326169045807976}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1932947272585538, 'mae': 0.13276925759015956}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19664447463824777, 'mae': 0.1343773762370641}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.194682154073551, 'mae': 0.1333565644992487}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19339962962623342, 'mae': 0.13278463511264177}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1935406529170259, 'mae': 0.13287502307652704}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20239770095179233, 'mae': 0.13746477751535222}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19782122651501816, 'mae': 0.13488979585574076}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19661313505224065, 'mae': 0.1347164125243539}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19483360894084195, 'mae': 0.133571543487462}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.20622698842694245, 'mae': 0.14010351287967335}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.2002200915743487, 'mae': 0.13669944857683253}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19801970196291926, 'mae': 0.13549542092415331}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1958169741721597, 'mae': 0.13416021044083426}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.20842210419773974, 'mae': 0.14078378463440938}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.20230427890881839, 'mae': 0.13776266106724325}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19920164544578003, 'mae': 0.13630153756448496}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19714180809298237, 'mae': 0.13500939662157155}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.21065130655807396, 'mae': 0.1420349987138404}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.20404160855337042, 'mae': 0.13901762300214798}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.1999539919260077, 'mae': 0.13684446587464022}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19827857391704834, 'mae': 0.13574333725402443}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.22400132459518213, 'mae': 0.14440245132713414}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.21060907740373908, 'mae': 0.14135253510592322}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2021595527958092, 'mae': 0.137996896548124}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.202203779942191, 'mae': 0.13797483310618636}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.23589491441287436, 'mae': 0.14852958107695144}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.21632190604979393, 'mae': 0.14591156362457472}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.205347052100211, 'mae': 0.14009504659314875}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.20470384360260632, 'mae': 0.139928368670493}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.24325770044916845, 'mae': 0.1501078275752437}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2216774373757654, 'mae': 0.14868013493909993}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.20842473044899223, 'mae': 0.1425830710656691}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.2070013591158127, 'mae': 0.14147960877444227}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.25680307783279255, 'mae': 0.15446550480491672}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.22656033140870993, 'mae': 0.15196362251918888}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.21097227096453935, 'mae': 0.1441018517827294}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.2092940781650065, 'mae': 0.143302848477306}\n"
     ]
    }
   ],
   "source": [
    "best_rmse=1000\n",
    "best_mae=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse) :\n",
    "                if(mae < best_mae) :\n",
    "                    best_rmse = rmse\n",
    "                    best_mae = mae\n",
    "                    best_learning_rate = rate\n",
    "                    best_n_estimators = estimator\n",
    "                    best_subsample = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19237995453006587"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13237502045293073"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07176005541202105"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['no_of_reviews'].corr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'reviewerName', 'reviewText', 'user_deviation',\n",
       "       'user_delay', 'helpful_votes', 'total_votes', 'no_of_reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09454703111241307"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['helpful_votes'].corr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08271041184733528"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['total_votes'].corr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0710923717070309"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(user_df['total_votes']-user_df['helpful_votes']).corr(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the All Review Helpfulness Variables - 23rd April 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'all_no_of_reviews', 'stem_sim_length', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation','user_delay','all_no_of_reviews']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count         wps  \\\n",
       "0              14.000000       176.92               1  427.000000   \n",
       "1               8.666667       175.64               2  423.000000   \n",
       "2               7.666667        13.01              23   19.521739   \n",
       "3               7.200000        14.00               5   12.800000   \n",
       "4               4.400000        61.55               1  138.000000   \n",
       "\n",
       "   review_length  pos_no  neg_no  user_deviation  user_delay  \\\n",
       "0            427       2       4        1.399189  10152000.0   \n",
       "1            846      18       8        0.624437  10886400.0   \n",
       "2            449       7       6        0.510375   7516800.0   \n",
       "3             64       2       0        0.442171   2678400.0   \n",
       "4            138       1       3        1.616074   7603200.0   \n",
       "\n",
       "   all_no_of_reviews  stem_sim_length  lem_sim_length  overall  \n",
       "0                  4               15              13      1.0  \n",
       "1                 11               24              23      3.0  \n",
       "2                 21               17              16      2.0  \n",
       "3                  6                2               2      5.0  \n",
       "4                  5                2               2      1.0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.037199865040779295\n",
      "Root Mean Squared Error (RMSE): 0.1928726653540602\n",
      "Mean Absolute Error (MAE):      0.13295202548339674\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08424164406873362"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['all_total_votes'].corr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08305330135835792"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['all_no_of_reviews'].corr(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.01,0.09,0.1,0.2,0.5,0.9], \n",
    "    'n_estimators': [200,300,400,500], \n",
    "    'subsample': [0.3, 0.5, 0.9, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'all_no_of_reviews', 'stem_sim_length', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2012084159554802, 'mae': 0.15557619679967602}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.2014640776183263, 'mae': 0.15579278624431794}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20168724868842774, 'mae': 0.15594298473079482}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20173107000583726, 'mae': 0.15598412014079477}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19534148559447317, 'mae': 0.14077125605515162}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19558310147023675, 'mae': 0.14097292819348267}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1957797949956035, 'mae': 0.14116391862237285}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19585855784714698, 'mae': 0.1412323518342349}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19397569225276587, 'mae': 0.13595140654975438}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19413689076117768, 'mae': 0.13610605318073457}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19434466991617697, 'mae': 0.13637828489497938}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19442676874058487, 'mae': 0.1364876234178403}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19347593927403592, 'mae': 0.1341456822564133}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19357098581646212, 'mae': 0.13430209076164012}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19377118041979755, 'mae': 0.1346023076719013}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19385214693686517, 'mae': 0.13471796541321243}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19326447933235474, 'mae': 0.1329263595313442}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19283961800813412, 'mae': 0.1326659240890211}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19269344060612226, 'mae': 0.13265376020055855}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19264432063942805, 'mae': 0.13262528068151203}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19316816849008356, 'mae': 0.1331030357967092}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19270798100612144, 'mae': 0.13264371475108508}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19257396568253285, 'mae': 0.13253410921326722}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19262587450501018, 'mae': 0.1325219576885926}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19337199063523414, 'mae': 0.13303211012942884}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19279456227319594, 'mae': 0.13255269448138424}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19256150418330997, 'mae': 0.13238962309019636}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1927200572530145, 'mae': 0.13253415198189805}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19347621339716536, 'mae': 0.1329976603104181}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19287006776682752, 'mae': 0.13250847543163724}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19263782917151073, 'mae': 0.13238724946714245}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1926882018367405, 'mae': 0.13247662080552083}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19303750258857552, 'mae': 0.132870851415387}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19272559473662726, 'mae': 0.13253020305198537}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19268796566763893, 'mae': 0.13263078634340025}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19263551922556368, 'mae': 0.13258713480344703}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19297810751018823, 'mae': 0.13314083249968564}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1926641078767454, 'mae': 0.13260740170664245}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19257610772606104, 'mae': 0.13254700911490386}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19264002501885694, 'mae': 0.13252715397108858}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19317807058683606, 'mae': 0.13306943279543273}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.1927164359345798, 'mae': 0.1323700453016501}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19258874253471878, 'mae': 0.13241330451911032}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19265186233890408, 'mae': 0.13251458153406892}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19322303489285947, 'mae': 0.13287846259614544}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.1927868930148273, 'mae': 0.1323181972528575}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19263180096669652, 'mae': 0.1323950507672861}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1926202522192121, 'mae': 0.13245079038753438}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19458352900528025, 'mae': 0.13370767558585603}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19334352398032467, 'mae': 0.13271555753021683}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19274548410925105, 'mae': 0.13245346495250138}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1927846335920185, 'mae': 0.13261724000175307}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19533696493408823, 'mae': 0.13434706447410802}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19365643151285164, 'mae': 0.13312110129603277}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19291028286455966, 'mae': 0.13252973570316323}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1929235977260254, 'mae': 0.1325996220148528}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1961710599254581, 'mae': 0.1345614980366836}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19433447049286037, 'mae': 0.1332635533013351}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19306434836121275, 'mae': 0.13247827435041504}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1932097685580967, 'mae': 0.13273701736232074}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.1968786560732032, 'mae': 0.13481942821993387}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.194684158325784, 'mae': 0.13343557124720765}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19333658773959234, 'mae': 0.13262934737617083}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1933625205367435, 'mae': 0.1328417706151438}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2024949522540201, 'mae': 0.13774093462593223}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19867967323565205, 'mae': 0.13578260027006395}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.1958949449179645, 'mae': 0.13408909192018012}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19557304957601584, 'mae': 0.13408105771928489}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.2055527714141595, 'mae': 0.1398431414664152}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.20034210594233554, 'mae': 0.13691883404201505}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19734710537397823, 'mae': 0.13516078267804685}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1966491068663978, 'mae': 0.13473626798330915}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.20955803005181836, 'mae': 0.14144782814355183}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.202111073203792, 'mae': 0.1380991870845755}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19859410704190134, 'mae': 0.13563342714191595}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1980453484674174, 'mae': 0.13554675332150204}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.21218062549136688, 'mae': 0.14320363451226975}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.20422499849610984, 'mae': 0.1393146124323224}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19974668192053346, 'mae': 0.1365750086337717}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1988366638404831, 'mae': 0.1361697322456396}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2216881773072779, 'mae': 0.14448420283490868}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.21007011544137172, 'mae': 0.14100353743547214}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2025589212774946, 'mae': 0.13790663310923432}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20194850983637627, 'mae': 0.13807342631462474}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.2368428120748824, 'mae': 0.1503513671504599}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.21675058243774334, 'mae': 0.14559962850907893}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2061555411654585, 'mae': 0.14000090991263028}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.20442767330563458, 'mae': 0.13991850640910547}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.24642037555047516, 'mae': 0.15200113044143876}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.22181101079383278, 'mae': 0.14853343068502348}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.20863301188244368, 'mae': 0.14222236639832084}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.20682912411944057, 'mae': 0.14166616436561524}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.259954543781913, 'mae': 0.15523137005065674}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.22519980935592898, 'mae': 0.15111228325816628}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.21088234600116876, 'mae': 0.14414256006263607}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.2086428449379412, 'mae': 0.14319452095921936}\n"
     ]
    }
   ],
   "source": [
    "best_rmse_2=1000\n",
    "best_mae_2=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_2) :\n",
    "                if(mae < best_mae_2) :\n",
    "                    best_rmse_2 = rmse\n",
    "                    best_mae_2 = mae\n",
    "                    best_learning_rate_2 = rate\n",
    "                    best_n_estimators_2 = estimator\n",
    "                    best_subsample_2 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19256150418330997"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.800000\n",
       "1    0.955556\n",
       "2    0.900000\n",
       "3    0.789474\n",
       "4    0.444444\n",
       "Name: helpfulness_score, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-51fc6bf6cb7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for abs(): 'list'"
     ]
    }
   ],
   "source": [
    "abs([-10,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trishul\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y2 = y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8       , 0.95555556, 0.9       , 0.78947368])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89840955, 0.9178419 , 0.9090657 , 0.9037471 ], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8       , 0.95555556, 0.9       , ..., 0.63636364, 0.41666667,\n",
       "       0.7826087 ])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'stem_sim_length', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation','user_delay','no_of_reviews']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count         wps  \\\n",
       "0              14.000000       176.92               1  427.000000   \n",
       "1               8.666667       175.64               2  423.000000   \n",
       "2               7.666667        13.01              23   19.521739   \n",
       "3               7.200000        14.00               5   12.800000   \n",
       "4               4.400000        61.55               1  138.000000   \n",
       "\n",
       "   review_length  pos_no  neg_no  user_deviation  user_delay  no_of_reviews  \\\n",
       "0            427       2       4        1.399189  10152000.0              1   \n",
       "1            846      18       8        0.624437  10886400.0              1   \n",
       "2            449       7       6        0.510375   7516800.0              1   \n",
       "3             64       2       0        0.442171   2678400.0              1   \n",
       "4            138       1       3        1.616074   7603200.0              1   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(learning_rate=best_learning_rate, n_estimators=best_n_estimators, subsample=best_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=400,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03701004690499021\n",
      "Root Mean Squared Error (RMSE): 0.19237995453006587\n",
      "Mean Absolute Error (MAE):      0.13237502045293073\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df['y_test']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51704</th>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.934962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29896</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.911354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.931369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77612</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_test  predictions\n",
       "51704  0.931034     0.934962\n",
       "29896  0.975610     0.911354\n",
       "9419   0.909091     0.931369\n",
       "77612  1.000000     0.873410\n",
       "7954   1.000000     0.874662"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df['difference'] = res_df['y_test'] - res_df['predictions'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df['error_abs'] = res_df['difference'].apply(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>predictions</th>\n",
       "      <th>difference</th>\n",
       "      <th>error_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51704</th>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.934962</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>0.003928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29896</th>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.911354</td>\n",
       "      <td>0.064256</td>\n",
       "      <td>0.064256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.931369</td>\n",
       "      <td>-0.022278</td>\n",
       "      <td>0.022278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77612</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873410</td>\n",
       "      <td>0.126590</td>\n",
       "      <td>0.126590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7954</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874662</td>\n",
       "      <td>0.125338</td>\n",
       "      <td>0.125338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_test  predictions  difference  error_abs\n",
       "51704  0.931034     0.934962   -0.003928   0.003928\n",
       "29896  0.975610     0.911354    0.064256   0.064256\n",
       "9419   0.909091     0.931369   -0.022278   0.022278\n",
       "77612  1.000000     0.873410    0.126590   0.126590\n",
       "7954   1.000000     0.874662    0.125338   0.125338"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>predictions</th>\n",
       "      <th>difference</th>\n",
       "      <th>error_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051973</td>\n",
       "      <td>1.051973</td>\n",
       "      <td>1.051973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69520</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917926</td>\n",
       "      <td>-0.917926</td>\n",
       "      <td>0.917926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61109</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916027</td>\n",
       "      <td>-0.916027</td>\n",
       "      <td>0.916027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915319</td>\n",
       "      <td>-0.915319</td>\n",
       "      <td>0.915319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74862</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908788</td>\n",
       "      <td>-0.908788</td>\n",
       "      <td>0.908788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26938</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>-0.906906</td>\n",
       "      <td>0.906906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57389</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904466</td>\n",
       "      <td>-0.904466</td>\n",
       "      <td>0.904466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62249</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.928113</td>\n",
       "      <td>-0.901086</td>\n",
       "      <td>0.901086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900834</td>\n",
       "      <td>-0.900834</td>\n",
       "      <td>0.900834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22696</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897455</td>\n",
       "      <td>-0.897455</td>\n",
       "      <td>0.897455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30212</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897221</td>\n",
       "      <td>-0.897221</td>\n",
       "      <td>0.897221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30207</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894968</td>\n",
       "      <td>-0.894968</td>\n",
       "      <td>0.894968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71653</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894308</td>\n",
       "      <td>-0.894308</td>\n",
       "      <td>0.894308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15368</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893959</td>\n",
       "      <td>-0.893959</td>\n",
       "      <td>0.893959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891777</td>\n",
       "      <td>-0.891777</td>\n",
       "      <td>0.891777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61105</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.890996</td>\n",
       "      <td>-0.890996</td>\n",
       "      <td>0.890996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37163</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887040</td>\n",
       "      <td>-0.887040</td>\n",
       "      <td>0.887040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36783</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884692</td>\n",
       "      <td>-0.884692</td>\n",
       "      <td>0.884692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75408</th>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.893363</td>\n",
       "      <td>-0.877234</td>\n",
       "      <td>0.877234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17033</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.872534</td>\n",
       "      <td>-0.872534</td>\n",
       "      <td>0.872534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871201</td>\n",
       "      <td>-0.871201</td>\n",
       "      <td>0.871201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8320</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870870</td>\n",
       "      <td>-0.870870</td>\n",
       "      <td>0.870870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48132</th>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.937014</td>\n",
       "      <td>-0.865072</td>\n",
       "      <td>0.865072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56194</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.930680</td>\n",
       "      <td>-0.864014</td>\n",
       "      <td>0.864014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81257</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861515</td>\n",
       "      <td>-0.861515</td>\n",
       "      <td>0.861515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63966</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.860972</td>\n",
       "      <td>-0.860972</td>\n",
       "      <td>0.860972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25109</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.921151</td>\n",
       "      <td>-0.854485</td>\n",
       "      <td>0.854485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64758</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.908433</td>\n",
       "      <td>-0.845933</td>\n",
       "      <td>0.845933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57243</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.943215</td>\n",
       "      <td>-0.843215</td>\n",
       "      <td>0.843215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20675</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.841685</td>\n",
       "      <td>-0.841685</td>\n",
       "      <td>0.841685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72968</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916819</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538310</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73539</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866810</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64077</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899869</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33144</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916535</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69058</th>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.783621</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76014</th>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.869680</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11133</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.922962</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55261</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916774</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64162</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.908985</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25432</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899899</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62118</th>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45138</th>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.924334</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57321</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928649</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29556</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909168</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20084</th>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.945519</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33173</th>\n",
       "      <td>0.936219</td>\n",
       "      <td>0.936277</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52183</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.912987</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74347</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.649964</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79249</th>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884647</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.941147</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61364</th>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.903250</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44100</th>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884598</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28009</th>\n",
       "      <td>0.937984</td>\n",
       "      <td>0.937997</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69354</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933346</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23849</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909082</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.927281</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72941</th>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.913049</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57215</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40667</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933331</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32510 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_test  predictions  difference  error_abs\n",
       "14416  1.000000    -0.051973    1.051973   1.051973\n",
       "69520  0.000000     0.917926   -0.917926   0.917926\n",
       "61109  0.000000     0.916027   -0.916027   0.916027\n",
       "42479  0.000000     0.915319   -0.915319   0.915319\n",
       "74862  0.000000     0.908788   -0.908788   0.908788\n",
       "26938  0.000000     0.906906   -0.906906   0.906906\n",
       "57389  0.000000     0.904466   -0.904466   0.904466\n",
       "62249  0.027027     0.928113   -0.901086   0.901086\n",
       "5371   0.000000     0.900834   -0.900834   0.900834\n",
       "22696  0.000000     0.897455   -0.897455   0.897455\n",
       "30212  0.000000     0.897221   -0.897221   0.897221\n",
       "30207  0.000000     0.894968   -0.894968   0.894968\n",
       "71653  0.000000     0.894308   -0.894308   0.894308\n",
       "15368  0.000000     0.893959   -0.893959   0.893959\n",
       "30208  0.000000     0.891777   -0.891777   0.891777\n",
       "61105  0.000000     0.890996   -0.890996   0.890996\n",
       "37163  0.000000     0.887040   -0.887040   0.887040\n",
       "36783  0.000000     0.884692   -0.884692   0.884692\n",
       "75408  0.016129     0.893363   -0.877234   0.877234\n",
       "17033  0.000000     0.872534   -0.872534   0.872534\n",
       "64005  0.000000     0.871201   -0.871201   0.871201\n",
       "8320   0.000000     0.870870   -0.870870   0.870870\n",
       "48132  0.071942     0.937014   -0.865072   0.865072\n",
       "56194  0.066667     0.930680   -0.864014   0.864014\n",
       "81257  0.000000     0.861515   -0.861515   0.861515\n",
       "63966  0.000000     0.860972   -0.860972   0.860972\n",
       "25109  0.066667     0.921151   -0.854485   0.854485\n",
       "64758  0.062500     0.908433   -0.845933   0.845933\n",
       "57243  0.100000     0.943215   -0.843215   0.843215\n",
       "20675  0.000000     0.841685   -0.841685   0.841685\n",
       "...         ...          ...         ...        ...\n",
       "72968  0.916667     0.916819   -0.000152   0.000152\n",
       "13     0.538462     0.538310    0.000152   0.000152\n",
       "73539  0.866667     0.866810   -0.000143   0.000143\n",
       "64077  0.900000     0.899869    0.000131   0.000131\n",
       "33144  0.916667     0.916535    0.000131   0.000131\n",
       "69058  0.783505     0.783621   -0.000116   0.000116\n",
       "76014  0.869565     0.869680   -0.000115   0.000115\n",
       "11133  0.923077     0.922962    0.000114   0.000114\n",
       "55261  0.916667     0.916774   -0.000108   0.000108\n",
       "64162  0.909091     0.908985    0.000106   0.000106\n",
       "25432  0.900000     0.899899    0.000101   0.000101\n",
       "62118  0.907692     0.907600    0.000092   0.000092\n",
       "45138  0.924242     0.924334   -0.000091   0.000091\n",
       "57321  0.928571     0.928649   -0.000077   0.000077\n",
       "29556  0.909091     0.909168   -0.000077   0.000077\n",
       "20084  0.945455     0.945519   -0.000064   0.000064\n",
       "33173  0.936219     0.936277   -0.000058   0.000058\n",
       "52183  0.913043     0.912987    0.000057   0.000057\n",
       "74347  0.650000     0.649964    0.000036   0.000036\n",
       "79249  0.884615     0.884647   -0.000032   0.000032\n",
       "3902   0.941176     0.941147    0.000029   0.000029\n",
       "61364  0.903226     0.903250   -0.000024   0.000024\n",
       "44100  0.884615     0.884598    0.000017   0.000017\n",
       "28009  0.937984     0.937997   -0.000013   0.000013\n",
       "69354  0.933333     0.933346   -0.000012   0.000012\n",
       "23849  0.909091     0.909082    0.000009   0.000009\n",
       "4834   0.927273     0.927281   -0.000008   0.000008\n",
       "72941  0.913043     0.913049   -0.000005   0.000005\n",
       "57215  0.928571     0.928567    0.000004   0.000004\n",
       "40667  0.933333     0.933331    0.000002   0.000002\n",
       "\n",
       "[32510 rows x 4 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(by=['error_abs'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>all_helpful_votes</th>\n",
       "      <th>all_total_votes</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QH8VQDE7HZCR</td>\n",
       "      <td>costaricachris</td>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FGQVJM18OWV</td>\n",
       "      <td>George S. Mitchell \"gsmitchell\"</td>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                     reviewerName  \\\n",
       "0   AMO214LNFCEI4                  Amazon Customer   \n",
       "1  A3N7T0DY83Y4IG                    C. A. Freeman   \n",
       "2  A1H8PY3QHMQQA0         Dave M. Shaw \"mack dave\"   \n",
       "3  A3QH8VQDE7HZCR                   costaricachris   \n",
       "4  A38FGQVJM18OWV  George S. Mitchell \"gsmitchell\"   \n",
       "\n",
       "                                          reviewText  user_deviation  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...        1.399189   \n",
       "1  Well, what can I say.  I've had this unit in m...        0.624437   \n",
       "2  Not going to write a long review, even thought...        0.510375   \n",
       "3  Quality was excellent. Instructions were clear...        0.442171   \n",
       "4  I checked around Amazon as well as some other ...        1.616074   \n",
       "\n",
       "   user_delay  helpful_votes  total_votes  no_of_reviews  all_helpful_votes  \\\n",
       "0  10152000.0             12           15              1                 12   \n",
       "1  10886400.0             11           11              1                 15   \n",
       "2   7516800.0             17           18              1                 24   \n",
       "3   2678400.0             15           19              1                 17   \n",
       "4   7603200.0              8           18              1                  8   \n",
       "\n",
       "   all_total_votes  all_no_of_reviews  \n",
       "0               15                  4  \n",
       "1               21                 11  \n",
       "2               30                 21  \n",
       "3               24                  6  \n",
       "4               18                  5  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_help=user_df['helpful_votes']/user_df['no_of_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12.0\n",
       "1    11.0\n",
       "2    17.0\n",
       "3    15.0\n",
       "4     8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_help.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1161965347854301"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_help.corr(review_df['helpfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total votes correlation:  0.0733968360582888\n",
      "all helfpul correlation:  0.08895267631661546\n",
      "all total correlation:  0.06005336563686367\n"
     ]
    }
   ],
   "source": [
    "avg_total = user_df['total_votes']/user_df['no_of_reviews']\n",
    "all_avg_help = user_df['all_helpful_votes']/user_df['all_no_of_reviews']\n",
    "all_avg_total = user_df['all_total_votes']/user_df['all_no_of_reviews']\n",
    "\n",
    "print(\"total votes correlation: \",avg_total.corr(review_df['helpfulness_score']))\n",
    "print(\"all helfpul correlation: \",all_avg_help.corr(review_df['helpfulness_score']))\n",
    "print(\"all total correlation: \",all_avg_total.corr(review_df['helpfulness_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hscore = review_df['helpfulness_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23112491522909748"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_avg_total - all_avg_help).corr(hscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['avg_helpful_votes'] = avg_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['unhelfpul_votes'] = user_df['total_votes'] - user_df['helpful_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_unhelpful = user_df['unhelfpul_votes']/user_df['no_of_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3725002117545532"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_unhelpful.corr(hscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1161965347854301"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_help.corr(hscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['avg_unhelpful_votes'] = avg_unhelpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['all_unhelpful_votes'] = user_df['all_total_votes'] - user_df['all_helpful_votes'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['all_avg_helpful_votes'] = all_avg_help\n",
    "user_df['all_avg_unhelpful_votes'] = user_df['all_unhelpful_votes']/user_df['all_no_of_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>all_helpful_votes</th>\n",
       "      <th>all_total_votes</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>avg_helpful_votes</th>\n",
       "      <th>unhelfpul_votes</th>\n",
       "      <th>avg_unhelpful_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QH8VQDE7HZCR</td>\n",
       "      <td>costaricachris</td>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FGQVJM18OWV</td>\n",
       "      <td>George S. Mitchell \"gsmitchell\"</td>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                     reviewerName  \\\n",
       "0   AMO214LNFCEI4                  Amazon Customer   \n",
       "1  A3N7T0DY83Y4IG                    C. A. Freeman   \n",
       "2  A1H8PY3QHMQQA0         Dave M. Shaw \"mack dave\"   \n",
       "3  A3QH8VQDE7HZCR                   costaricachris   \n",
       "4  A38FGQVJM18OWV  George S. Mitchell \"gsmitchell\"   \n",
       "\n",
       "                                          reviewText  user_deviation  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...        1.399189   \n",
       "1  Well, what can I say.  I've had this unit in m...        0.624437   \n",
       "2  Not going to write a long review, even thought...        0.510375   \n",
       "3  Quality was excellent. Instructions were clear...        0.442171   \n",
       "4  I checked around Amazon as well as some other ...        1.616074   \n",
       "\n",
       "   user_delay  helpful_votes  total_votes  no_of_reviews  all_helpful_votes  \\\n",
       "0  10152000.0             12           15              1                 12   \n",
       "1  10886400.0             11           11              1                 15   \n",
       "2   7516800.0             17           18              1                 24   \n",
       "3   2678400.0             15           19              1                 17   \n",
       "4   7603200.0              8           18              1                  8   \n",
       "\n",
       "   all_total_votes  all_no_of_reviews  avg_helpful_votes  unhelfpul_votes  \\\n",
       "0               15                  4               12.0                3   \n",
       "1               21                 11               11.0                0   \n",
       "2               30                 21               17.0                1   \n",
       "3               24                  6               15.0                4   \n",
       "4               18                  5                8.0               10   \n",
       "\n",
       "   avg_unhelpful_votes  \n",
       "0                  3.0  \n",
       "1                  0.0  \n",
       "2                  1.0  \n",
       "3                  4.0  \n",
       "4                 10.0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with avg helpful and avg unhelpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'stem_sim_length', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count         wps  \\\n",
       "0              14.000000       176.92               1  427.000000   \n",
       "1               8.666667       175.64               2  423.000000   \n",
       "2               7.666667        13.01              23   19.521739   \n",
       "3               7.200000        14.00               5   12.800000   \n",
       "4               4.400000        61.55               1  138.000000   \n",
       "\n",
       "   review_length  pos_no  neg_no  user_deviation  user_delay  no_of_reviews  \\\n",
       "0            427       2       4        1.399189  10152000.0              1   \n",
       "1            846      18       8        0.624437  10886400.0              1   \n",
       "2            449       7       6        0.510375   7516800.0              1   \n",
       "3             64       2       0        0.442171   2678400.0              1   \n",
       "4            138       1       3        1.616074   7603200.0              1   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.037214149448659396\n",
      "Root Mean Squared Error (RMSE): 0.1929096924694542\n",
      "Mean Absolute Error (MAE):      0.13303612335141038\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Individual Feature Prediction - avg helpful, avg unhelpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>all_helpful_votes</th>\n",
       "      <th>all_total_votes</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>avg_helpful_votes</th>\n",
       "      <th>unhelfpul_votes</th>\n",
       "      <th>avg_unhelpful_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QH8VQDE7HZCR</td>\n",
       "      <td>costaricachris</td>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FGQVJM18OWV</td>\n",
       "      <td>George S. Mitchell \"gsmitchell\"</td>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                     reviewerName  \\\n",
       "0   AMO214LNFCEI4                  Amazon Customer   \n",
       "1  A3N7T0DY83Y4IG                    C. A. Freeman   \n",
       "2  A1H8PY3QHMQQA0         Dave M. Shaw \"mack dave\"   \n",
       "3  A3QH8VQDE7HZCR                   costaricachris   \n",
       "4  A38FGQVJM18OWV  George S. Mitchell \"gsmitchell\"   \n",
       "\n",
       "                                          reviewText  user_deviation  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...        1.399189   \n",
       "1  Well, what can I say.  I've had this unit in m...        0.624437   \n",
       "2  Not going to write a long review, even thought...        0.510375   \n",
       "3  Quality was excellent. Instructions were clear...        0.442171   \n",
       "4  I checked around Amazon as well as some other ...        1.616074   \n",
       "\n",
       "   user_delay  helpful_votes  total_votes  no_of_reviews  all_helpful_votes  \\\n",
       "0  10152000.0             12           15              1                 12   \n",
       "1  10886400.0             11           11              1                 15   \n",
       "2   7516800.0             17           18              1                 24   \n",
       "3   2678400.0             15           19              1                 17   \n",
       "4   7603200.0              8           18              1                  8   \n",
       "\n",
       "   all_total_votes  all_no_of_reviews  avg_helpful_votes  unhelfpul_votes  \\\n",
       "0               15                  4               12.0                3   \n",
       "1               21                 11               11.0                0   \n",
       "2               30                 21               17.0                1   \n",
       "3               24                  6               15.0                4   \n",
       "4               18                  5                8.0               10   \n",
       "\n",
       "   avg_unhelpful_votes  \n",
       "0                  3.0  \n",
       "1                  0.0  \n",
       "2                  1.0  \n",
       "3                  4.0  \n",
       "4                 10.0  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = user_df['iota']\n",
    "X = np.array(X).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8125    ],\n",
       "       [1.        ],\n",
       "       [0.94736842],\n",
       "       ...,\n",
       "       [0.66666667],\n",
       "       [0.73267327],\n",
       "       [0.91037446]])"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.07494140050242103\n",
      "Root Mean Squared Error (RMSE): 0.27375427029075006\n",
      "Mean Absolute Error (MAE):      0.17365267622730043\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of all no of reviews and all avg unhelpful votes with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'all_no_of_reviews', 'all_avg_unhelpful_votes', 'stem_sim_length',\n",
       "       'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','all_no_of_reviews', 'all_avg_unhelpful_votes']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>all_avg_unhelpful_votes</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count   ...     review_length  \\\n",
       "0              14.000000       176.92               1   ...               427   \n",
       "1               8.666667       175.64               2   ...               846   \n",
       "2               7.666667        13.01              23   ...               449   \n",
       "3               7.200000        14.00               5   ...                64   \n",
       "4               4.400000        61.55               1   ...               138   \n",
       "\n",
       "   pos_no  neg_no  user_deviation  user_delay  all_no_of_reviews  \\\n",
       "0       2       4        1.399189  10152000.0                  4   \n",
       "1      18       8        0.624437  10886400.0                 11   \n",
       "2       7       6        0.510375   7516800.0                 21   \n",
       "3       2       0        0.442171   2678400.0                  6   \n",
       "4       1       3        1.616074   7603200.0                  5   \n",
       "\n",
       "   all_avg_unhelpful_votes  stem_sim_length  lem_sim_length  overall  \n",
       "0                 0.750000               15              13      1.0  \n",
       "1                 0.545455               24              23      3.0  \n",
       "2                 0.285714               17              16      2.0  \n",
       "3                 1.166667                2               2      5.0  \n",
       "4                 2.000000                2               2      1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.05027398806992936\n",
      "Root Mean Squared Error (RMSE): 0.22421861668900145\n",
      "Mean Absolute Error (MAE):      0.14249431558178208\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.01, 0.09, 0.1, 0.2, 0.5, 0.9],\n",
       " 'n_estimators': [200, 300, 400, 500],\n",
       " 'subsample': [0.3, 0.5, 0.9, 1]}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.21314307319297318, 'mae': 0.1530811937935574}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.2130614453650779, 'mae': 0.15319696240300948}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2134591035019029, 'mae': 0.15335639210365457}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.21355567276934975, 'mae': 0.15338899503756429}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.21410896802897203, 'mae': 0.14187605397699626}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.21416492237200602, 'mae': 0.14201952367572518}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.21432378581423026, 'mae': 0.14211416697756685}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.21436819762516715, 'mae': 0.14216507108874984}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.2160939108028178, 'mae': 0.13955571003007095}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.21632435116494417, 'mae': 0.1398351097478635}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.2163393206577669, 'mae': 0.1398200369130327}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.21645148705522768, 'mae': 0.13989166234291328}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.2176947095195329, 'mae': 0.13932711849062046}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.21782584428608334, 'mae': 0.13953724909853335}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.21813694827946767, 'mae': 0.13973270584794012}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.21831611069402362, 'mae': 0.13985560636368377}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.22413921633718525, 'mae': 0.14251250444609115}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.22588283581419058, 'mae': 0.14328510794796645}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2257778993648595, 'mae': 0.1434060348357343}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.22661860632934677, 'mae': 0.1438352926805586}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.22437644068789434, 'mae': 0.14252279197063839}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.22665324778367169, 'mae': 0.14355897650667807}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2264497626257543, 'mae': 0.14373521669075887}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.22749105464073308, 'mae': 0.1442891102921735}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.22529127372053023, 'mae': 0.1429351444439178}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2281248486720561, 'mae': 0.14436086613788202}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.22754400727006746, 'mae': 0.14431660464195903}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.22820973661843272, 'mae': 0.14460373774681012}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.22643183191011934, 'mae': 0.1434841559972804}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.22880116727410219, 'mae': 0.14465133629441765}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.22811516466554443, 'mae': 0.14457309441737434}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.22856040477539272, 'mae': 0.1447860700955245}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.22557505928023938, 'mae': 0.14302395459028155}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.22639583609705355, 'mae': 0.14353905074108841}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.22637529320957925, 'mae': 0.1438313861077191}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.22708630942961952, 'mae': 0.1441280303376727}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.22645946630710262, 'mae': 0.14353876434326587}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.22669168913139442, 'mae': 0.14372297521656915}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2269579202434638, 'mae': 0.14410918699821332}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.22789443541915827, 'mae': 0.14455005010030839}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.22805413813686132, 'mae': 0.1442967087365929}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2282838869892036, 'mae': 0.14454874931733544}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.22762912828151774, 'mae': 0.14439915413131912}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.22888799136581758, 'mae': 0.14502554428146502}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.2290021863044819, 'mae': 0.14484253417930895}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.22885059383642736, 'mae': 0.14486619572906684}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.22798416081306822, 'mae': 0.14460918007554707}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.22907185259282575, 'mae': 0.14511104677061648}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.22703550694750027, 'mae': 0.1443117692427512}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.2281542210166618, 'mae': 0.14458371635023282}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2287621965456846, 'mae': 0.1449111687477146}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.22917556242254608, 'mae': 0.14538664767745332}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.22766718629388105, 'mae': 0.14504713909396114}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.22892817819433536, 'mae': 0.14505069432308162}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2294912021899174, 'mae': 0.1451894200837261}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.2297016724866146, 'mae': 0.1456492641616206}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.22930722889312058, 'mae': 0.14568282335467048}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.23001537169250108, 'mae': 0.14564086220583947}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.2299497321143687, 'mae': 0.14543155573485336}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.23016595406327023, 'mae': 0.14588331190813691}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.22953282663446892, 'mae': 0.14573969253919347}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.2306267871373117, 'mae': 0.14609974464781986}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.23004500437901723, 'mae': 0.14557327808422907}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.23015996181639112, 'mae': 0.14593999848294775}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.23988985029301035, 'mae': 0.15223076732813656}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.232845621548695, 'mae': 0.14781240991396694}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2341107503362954, 'mae': 0.14864803679091304}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.23220775691331275, 'mae': 0.14750890594951319}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.2422596506891614, 'mae': 0.15380591412607023}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.2340720405390614, 'mae': 0.14884381685926987}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.23434873805907477, 'mae': 0.1489228580005637}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.2335598682009292, 'mae': 0.14848617701271025}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.24419434130371792, 'mae': 0.15528318624029008}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.23505788580097423, 'mae': 0.14962701798035338}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.23538462381110647, 'mae': 0.14978708470167548}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.23447526800187501, 'mae': 0.14924534441493936}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.24665709223527163, 'mae': 0.15716333033490573}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.2366029895764488, 'mae': 0.1509890877764277}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.2357544814928182, 'mae': 0.15010351556389964}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.23469542055862025, 'mae': 0.1495120211567962}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2523499612938835, 'mae': 0.1582707710838835}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.24063758170628766, 'mae': 0.15306212337398326}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.23959913541538994, 'mae': 0.15306605474632737}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.23815537783929894, 'mae': 0.15212386104243547}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.26220601019351564, 'mae': 0.1632723296621157}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.24452427408959812, 'mae': 0.15648723419303393}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.23978670928135604, 'mae': 0.15369342119297108}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.23993570269399037, 'mae': 0.15370590358424152}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.2750759272201572, 'mae': 0.16592834519473232}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.24849749100915264, 'mae': 0.1591149390362939}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.24047546063360373, 'mae': 0.15467031469444803}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.24099290708572654, 'mae': 0.15487261475292663}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.2917081399870458, 'mae': 0.16873302344686608}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.25066419139496277, 'mae': 0.1610350800001078}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.2414083265962583, 'mae': 0.15568975828719328}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.24189248266400065, 'mae': 0.15632993260607977}\n"
     ]
    }
   ],
   "source": [
    "best_rmse_2=1000\n",
    "best_mae_2=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_2) :\n",
    "                if(mae < best_mae_2) :\n",
    "                    best_rmse_2 = rmse\n",
    "                    best_mae_2 = mae\n",
    "                    best_learning_rate_2 = rate\n",
    "                    best_n_estimators_2 = estimator\n",
    "                    best_subsample_2 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21314307319297318"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1530811937935574"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mae_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment : Optimization , using feature subset based on correlation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['smog_index', 'wps', 'coleman_liau_index', 'linsear_write_formula',\n",
       "       'sentence_count', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[[ 'smog_index', 'wps', 'coleman_liau_index',\n",
    "       'linsear_write_formula', 'sentence_count', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews']])\n",
    "\n",
    "X = X.join(meta_df[['lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smog_index</th>\n",
       "      <th>wps</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>7.69</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.3</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.8</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>8.05</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>8.09</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   smog_index         wps  coleman_liau_index  linsear_write_formula  \\\n",
       "0         0.0  427.000000                7.69              14.000000   \n",
       "1         0.0  423.000000                9.72               8.666667   \n",
       "2         7.3   19.521739                5.92               7.666667   \n",
       "3        10.8   12.800000                8.05               7.200000   \n",
       "4         0.0  138.000000                8.09               4.400000   \n",
       "\n",
       "   sentence_count  pos_no  neg_no  user_deviation  user_delay  no_of_reviews  \\\n",
       "0               1       2       4        1.399189  10152000.0              1   \n",
       "1               2      18       8        0.624437  10886400.0              1   \n",
       "2              23       7       6        0.510375   7516800.0              1   \n",
       "3               5       2       0        0.442171   2678400.0              1   \n",
       "4               1       1       3        1.616074   7603200.0              1   \n",
       "\n",
       "   lem_sim_length  overall  \n",
       "0              13      1.0  \n",
       "1              23      3.0  \n",
       "2              16      2.0  \n",
       "3               2      5.0  \n",
       "4               2      1.0  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03754420894859956\n",
      "Root Mean Squared Error (RMSE): 0.1937632807025097\n",
      "Mean Absolute Error (MAE):      0.13367795600713675\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of User Early and Latest Review Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpfulness_score</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>review_delay</th>\n",
       "      <th>stem_words</th>\n",
       "      <th>lem_words</th>\n",
       "      <th>stem_sim_words</th>\n",
       "      <th>lem_sim_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1290643200</td>\n",
       "      <td>11 25, 2010</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>6652800</td>\n",
       "      <td>['im', 'profession', 'otr', 'truck', 'driver',...</td>\n",
       "      <td>['im', 'professional', 'otr', 'truck', 'driver...</td>\n",
       "      <td>['5']</td>\n",
       "      <td>['rand', 'route', 'weight', 'take', 'time', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1283990400</td>\n",
       "      <td>09 9, 2010</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', 'say', 'ive', 'unit', 'truck', 'four'...</td>\n",
       "      <td>['well', 'say', 'ive', 'unit', 'truck', 'four'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['dock', 'feature', 'car', 'determine', 'home'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1290556800</td>\n",
       "      <td>11 24, 2010</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>6566400</td>\n",
       "      <td>['go', 'write', 'long', 'review', 'even', 'tho...</td>\n",
       "      <td>['going', 'write', 'long', 'review', 'even', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['screen', 'road', 'rand', 'route', 'truck', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QH8VQDE7HZCR</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>costaricachris</td>\n",
       "      <td>[15, 19]</td>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Real value for the money</td>\n",
       "      <td>1286236800</td>\n",
       "      <td>10 5, 2010</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400</td>\n",
       "      <td>['qualiti', 'excel', 'instruct', 'clear', 'cle...</td>\n",
       "      <td>['quality', 'excellent', 'instruction', 'clear...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['level', 'hardware']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FGQVJM18OWV</td>\n",
       "      <td>0972683275</td>\n",
       "      <td>George S. Mitchell \"gsmitchell\"</td>\n",
       "      <td>[8, 18]</td>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What a piece of junk!</td>\n",
       "      <td>1291161600</td>\n",
       "      <td>12 1, 2010</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200</td>\n",
       "      <td>['check', 'around', 'amazon', 'well', 'site', ...</td>\n",
       "      <td>['checked', 'around', 'amazon', 'well', 'site'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['tv', 'mount']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                     reviewerName   helpful  \\\n",
       "0   AMO214LNFCEI4  0528881469                  Amazon Customer  [12, 15]   \n",
       "1  A3N7T0DY83Y4IG  0528881469                    C. A. Freeman  [43, 45]   \n",
       "2  A1H8PY3QHMQQA0  0528881469         Dave M. Shaw \"mack dave\"   [9, 10]   \n",
       "3  A3QH8VQDE7HZCR  0972683275                   costaricachris  [15, 19]   \n",
       "4  A38FGQVJM18OWV  0972683275  George S. Mitchell \"gsmitchell\"   [8, 18]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...      1.0   \n",
       "1  Well, what can I say.  I've had this unit in m...      3.0   \n",
       "2  Not going to write a long review, even thought...      2.0   \n",
       "3  Quality was excellent. Instructions were clear...      5.0   \n",
       "4  I checked around Amazon as well as some other ...      1.0   \n",
       "\n",
       "                    summary  unixReviewTime   reviewTime  helpfulness_score  \\\n",
       "0         Very Disappointed      1290643200  11 25, 2010           0.800000   \n",
       "1            1st impression      1283990400   09 9, 2010           0.955556   \n",
       "2   Great grafics, POOR GPS      1290556800  11 24, 2010           0.900000   \n",
       "3  Real value for the money      1286236800   10 5, 2010           0.789474   \n",
       "4     What a piece of junk!      1291161600   12 1, 2010           0.444444   \n",
       "\n",
       "   user_deviation  review_delay  \\\n",
       "0        1.399189       6652800   \n",
       "1        0.624437             0   \n",
       "2        0.510375       6566400   \n",
       "3        0.442171       2678400   \n",
       "4        1.616074       7603200   \n",
       "\n",
       "                                          stem_words  \\\n",
       "0  ['im', 'profession', 'otr', 'truck', 'driver',...   \n",
       "1  ['well', 'say', 'ive', 'unit', 'truck', 'four'...   \n",
       "2  ['go', 'write', 'long', 'review', 'even', 'tho...   \n",
       "3  ['qualiti', 'excel', 'instruct', 'clear', 'cle...   \n",
       "4  ['check', 'around', 'amazon', 'well', 'site', ...   \n",
       "\n",
       "                                           lem_words stem_sim_words  \\\n",
       "0  ['im', 'professional', 'otr', 'truck', 'driver...          ['5']   \n",
       "1  ['well', 'say', 'ive', 'unit', 'truck', 'four'...             []   \n",
       "2  ['going', 'write', 'long', 'review', 'even', '...             []   \n",
       "3  ['quality', 'excellent', 'instruction', 'clear...             []   \n",
       "4  ['checked', 'around', 'amazon', 'well', 'site'...             []   \n",
       "\n",
       "                                       lem_sim_words  \n",
       "0  ['rand', 'route', 'weight', 'take', 'time', 'l...  \n",
       "1  ['dock', 'feature', 'car', 'determine', 'home'...  \n",
       "2  ['screen', 'road', 'rand', 'route', 'truck', '...  \n",
       "3                              ['level', 'hardware']  \n",
       "4                                    ['tv', 'mount']  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewer = review_df.groupby(['reviewerID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewer1 = reviewer['unixReviewTime'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewer2 = reviewer['unixReviewTime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewer3 = reviewer2 - reviewer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewer4 = reviewer3/86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID\n",
       "A0103849GBVWICKXD4T6       0.0\n",
       "A0191512Q2Z9IPUAE2RZ       0.0\n",
       "A0266076X6KPZ6CCHGVS       0.0\n",
       "A05793033A673QTUT56XZ    338.0\n",
       "A09899451IM4ZNZEJQXHO      0.0\n",
       "Name: unixReviewTime, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewer4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviewer_days(reviewerID) :\n",
    "    return reviewer4.loc[reviewerID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['reviewer_days'] = user_df['reviewerID'].apply(get_reviewer_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>all_helpful_votes</th>\n",
       "      <th>all_total_votes</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>1390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3QH8VQDE7HZCR</td>\n",
       "      <td>costaricachris</td>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A38FGQVJM18OWV</td>\n",
       "      <td>George S. Mitchell \"gsmitchell\"</td>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                     reviewerName  \\\n",
       "0   AMO214LNFCEI4                  Amazon Customer   \n",
       "1  A3N7T0DY83Y4IG                    C. A. Freeman   \n",
       "2  A1H8PY3QHMQQA0         Dave M. Shaw \"mack dave\"   \n",
       "3  A3QH8VQDE7HZCR                   costaricachris   \n",
       "4  A38FGQVJM18OWV  George S. Mitchell \"gsmitchell\"   \n",
       "\n",
       "                                          reviewText  user_deviation  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...        1.399189   \n",
       "1  Well, what can I say.  I've had this unit in m...        0.624437   \n",
       "2  Not going to write a long review, even thought...        0.510375   \n",
       "3  Quality was excellent. Instructions were clear...        0.442171   \n",
       "4  I checked around Amazon as well as some other ...        1.616074   \n",
       "\n",
       "   user_delay  helpful_votes  total_votes  no_of_reviews  all_helpful_votes  \\\n",
       "0  10152000.0             12           15              1                 12   \n",
       "1  10886400.0             11           11              1                 15   \n",
       "2   7516800.0             17           18              1                 24   \n",
       "3   2678400.0             15           19              1                 17   \n",
       "4   7603200.0              8           18              1                  8   \n",
       "\n",
       "   all_total_votes  all_no_of_reviews  reviewer_days  \n",
       "0               15                  4          260.0  \n",
       "1               21                 11         1390.0  \n",
       "2               30                 21          929.0  \n",
       "3               24                  6            0.0  \n",
       "4               18                  5            0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df.to_csv('User_DF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51239"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_df[user_df['reviewer_days'] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4892931408087923"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['reviewer_days'].corr(user_df['no_of_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - usage of reviewer days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'reviewer_days', 'epsilon', 'stem_sim_length',\n",
       "       'lem_sim_length', 'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days','epsilon']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count   ...     pos_no  \\\n",
       "0              14.000000       176.92               1   ...          2   \n",
       "1               8.666667       175.64               2   ...         18   \n",
       "2               7.666667        13.01              23   ...          7   \n",
       "3               7.200000        14.00               5   ...          2   \n",
       "4               4.400000        61.55               1   ...          1   \n",
       "\n",
       "   neg_no  user_deviation  user_delay  no_of_reviews  reviewer_days   epsilon  \\\n",
       "0       4        1.399189  10152000.0              1          260.0  0.007663   \n",
       "1       8        0.624437  10886400.0              1         1390.0  0.001438   \n",
       "2       6        0.510375   7516800.0              1          929.0  0.002151   \n",
       "3       0        0.442171   2678400.0              1            0.0  2.000000   \n",
       "4       3        1.616074   7603200.0              1            0.0  2.000000   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03717392871490753\n",
      "Root Mean Squared Error (RMSE): 0.19280541671568133\n",
      "Mean Absolute Error (MAE):      0.1329168852217482\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-576-d2f0d49ce402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msubsample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subsample'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mxgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mrmse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Trishul\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    371\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Trishul\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Trishul\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Trishul\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1045\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_rmse_4=1000\n",
    "best_mae_4=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_4) :\n",
    "                if(mae < best_mae_4) :\n",
    "                    best_rmse_4 = rmse\n",
    "                    best_mae_4 = mae\n",
    "                    best_learning_rate_4 = rate\n",
    "                    best_n_estimators_4 = estimator\n",
    "                    best_subsample_4 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1920870022409736, 0.13192014023385665)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse_3,best_mae_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_learning_rate_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_estimators_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_subsample_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>helpfulness_score</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>-1th and 0th grade</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>427</td>\n",
       "      <td>['available', 'preferred']</td>\n",
       "      <td>['problem', 'refused', 'cheap', 'lost']</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>165th and 166th grade</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>846</td>\n",
       "      <td>['favorite', 'like', 'best', 'best', 'importan...</td>\n",
       "      <td>['strictly', 'slow', 'overwhelming', 'omission...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>85.52</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>5th and 6th grade</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>449</td>\n",
       "      <td>['well', 'pretty', 'fastest', 'excited', 'grea...</td>\n",
       "      <td>['trouble', 'lacking', 'mistakes', 'problem', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quality was excellent. Instructions were clear...</td>\n",
       "      <td>66.94</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>6th and 7th grade</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>64</td>\n",
       "      <td>['clear', 'enough']</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I checked around Amazon as well as some other ...</td>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>53th and 54th grade</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>138</td>\n",
       "      <td>['well']</td>\n",
       "      <td>['damage', 'issue', 'hard']</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  flesch_reading_ease  \\\n",
       "0  I'm a professional OTR truck driver, and I bou...              -336.56   \n",
       "1  Well, what can I say.  I've had this unit in m...              -340.96   \n",
       "2  Not going to write a long review, even thought...                85.52   \n",
       "3  Quality was excellent. Instructions were clear...                66.94   \n",
       "4  I checked around Amazon as well as some other ...               -43.22   \n",
       "\n",
       "   helpfulness_score  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0           0.800000         0.0                 166.3                7.69   \n",
       "1           0.955556         0.0                 165.9                9.72   \n",
       "2           0.900000         7.3                   6.2                5.92   \n",
       "3           0.789474        10.8                   7.1                8.05   \n",
       "4           0.444444         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog          text_standard  sentence_count  \\\n",
       "0              14.000000       176.92     -1th and 0th grade               1   \n",
       "1               8.666667       175.64  165th and 166th grade               2   \n",
       "2               7.666667        13.01      5th and 6th grade              23   \n",
       "3               7.200000        14.00      6th and 7th grade               5   \n",
       "4               4.400000        61.55    53th and 54th grade               1   \n",
       "\n",
       "          wps  review_length  \\\n",
       "0  427.000000            427   \n",
       "1  423.000000            846   \n",
       "2   19.521739            449   \n",
       "3   12.800000             64   \n",
       "4  138.000000            138   \n",
       "\n",
       "                                           pos_words  \\\n",
       "0                         ['available', 'preferred']   \n",
       "1  ['favorite', 'like', 'best', 'best', 'importan...   \n",
       "2  ['well', 'pretty', 'fastest', 'excited', 'grea...   \n",
       "3                                ['clear', 'enough']   \n",
       "4                                           ['well']   \n",
       "\n",
       "                                           neg_words  pos_no  neg_no  \n",
       "0            ['problem', 'refused', 'cheap', 'lost']       2       4  \n",
       "1  ['strictly', 'slow', 'overwhelming', 'omission...      18       8  \n",
       "2  ['trouble', 'lacking', 'mistakes', 'problem', ...       7       6  \n",
       "3                                                 []       2       0  \n",
       "4                        ['damage', 'issue', 'hard']       1       3  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>all_helpful_votes</th>\n",
       "      <th>all_total_votes</th>\n",
       "      <th>all_no_of_reviews</th>\n",
       "      <th>avg_helpful_votes</th>\n",
       "      <th>unhelfpul_votes</th>\n",
       "      <th>avg_unhelpful_votes</th>\n",
       "      <th>all_unhelpful_votes</th>\n",
       "      <th>all_avg_helpful_votes</th>\n",
       "      <th>all_avg_unhelpful_votes</th>\n",
       "      <th>reviewer_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [reviewerID, reviewerName, reviewText, user_deviation, user_delay, helpful_votes, total_votes, no_of_reviews, all_helpful_votes, all_total_votes, all_no_of_reviews, avg_helpful_votes, unhelfpul_votes, avg_unhelpful_votes, all_unhelpful_votes, all_avg_helpful_votes, all_avg_unhelpful_votes, reviewer_days]\n",
       "Index: []"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df[user_df['avg_unhelpful_votes']==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81275"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_df['avg_unhelpful_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'reviewerName', 'reviewText', 'user_deviation',\n",
       "       'user_delay', 'helpful_votes', 'total_votes', 'no_of_reviews',\n",
       "       'all_helpful_votes', 'all_total_votes', 'all_no_of_reviews',\n",
       "       'avg_helpful_votes', 'unhelfpul_votes', 'avg_unhelpful_votes',\n",
       "       'all_unhelpful_votes', 'all_avg_helpful_votes',\n",
       "       'all_avg_unhelpful_votes', 'reviewer_days'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['unhelpful_votes'] = user_df['total_votes'] - user_df['helpful_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81275"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['unhelpful_votes'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['avg_unhelpful_votes_v2']=user_df['avg_unhelpful_votes'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. alpha -> unhelpful / days\n",
    "2. beta  -> avg unhelpful / days\n",
    "3. gamma -> avg unhelpful v2 / days\n",
    "4. delta -> unhelpful / (reviews*days)\n",
    "5. iota  -> helpful/total\n",
    "6. epsilon -> reviews no / days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21871856699081108"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(user_df['unhelpful_votes']/(user_df['reviewer_days']+1)).corr(review_df['helpfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['alpha'] = user_df['unhelpful_votes']/(user_df['reviewer_days']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['beta'] = user_df['avg_unhelpful_votes']/(user_df['reviewer_days']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['gamma'] = user_df['avg_unhelpful_votes_v2']/(user_df['reviewer_days']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['delta'] = user_df['unhelpful_votes']/((user_df['no_of_reviews']+1)*(user_df['reviewer_days']+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['iota'] =(user_df['helpful_votes']+1)/(user_df['total_votes']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6024305529364564"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['iota'].corr(review_df['helpfulness_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_df['epsilon'] = (user_df['no_of_reviews']+1) / (user_df['reviewer_days']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07094621221965774"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['epsilon'].corr(review_df['helpfulness_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-Feature Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_corr=X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.321961</td>\n",
       "      <td>-0.999465</td>\n",
       "      <td>-0.260971</td>\n",
       "      <td>-0.999300</td>\n",
       "      <td>-0.996831</td>\n",
       "      <td>-0.355395</td>\n",
       "      <td>-0.133238</td>\n",
       "      <td>-0.999334</td>\n",
       "      <td>0.265178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355110</td>\n",
       "      <td>-0.318744</td>\n",
       "      <td>-0.299891</td>\n",
       "      <td>0.057596</td>\n",
       "      <td>-0.001563</td>\n",
       "      <td>-0.085232</td>\n",
       "      <td>-0.105965</td>\n",
       "      <td>-0.238218</td>\n",
       "      <td>-0.239106</td>\n",
       "      <td>-0.052626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>0.321961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.325246</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>-0.323973</td>\n",
       "      <td>-0.324201</td>\n",
       "      <td>0.363657</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>-0.327498</td>\n",
       "      <td>0.368345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337437</td>\n",
       "      <td>0.310563</td>\n",
       "      <td>0.283001</td>\n",
       "      <td>-0.066908</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.076553</td>\n",
       "      <td>0.215368</td>\n",
       "      <td>0.215090</td>\n",
       "      <td>0.064401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <td>-0.999465</td>\n",
       "      <td>-0.325246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.236112</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>-0.264906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354533</td>\n",
       "      <td>0.317912</td>\n",
       "      <td>0.298891</td>\n",
       "      <td>-0.057829</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>0.235802</td>\n",
       "      <td>0.236770</td>\n",
       "      <td>0.052886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <td>-0.260971</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.236112</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241079</td>\n",
       "      <td>0.281476</td>\n",
       "      <td>0.280264</td>\n",
       "      <td>0.282248</td>\n",
       "      <td>0.238779</td>\n",
       "      <td>-0.068969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152129</td>\n",
       "      <td>0.148471</td>\n",
       "      <td>0.142632</td>\n",
       "      <td>-0.024429</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.136980</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.147523</td>\n",
       "      <td>0.027022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>-0.999300</td>\n",
       "      <td>-0.323973</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>0.241079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995590</td>\n",
       "      <td>0.351371</td>\n",
       "      <td>0.131895</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>-0.263448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355621</td>\n",
       "      <td>0.318982</td>\n",
       "      <td>0.299865</td>\n",
       "      <td>-0.057983</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.083146</td>\n",
       "      <td>0.103518</td>\n",
       "      <td>0.236302</td>\n",
       "      <td>0.237283</td>\n",
       "      <td>0.053202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <td>-0.996831</td>\n",
       "      <td>-0.324201</td>\n",
       "      <td>0.995617</td>\n",
       "      <td>0.281476</td>\n",
       "      <td>0.995590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348506</td>\n",
       "      <td>0.135912</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>-0.274865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339885</td>\n",
       "      <td>0.306564</td>\n",
       "      <td>0.288581</td>\n",
       "      <td>-0.057995</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>0.085654</td>\n",
       "      <td>0.105880</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.233856</td>\n",
       "      <td>0.054906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difficult_words</th>\n",
       "      <td>-0.355395</td>\n",
       "      <td>0.363657</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.280264</td>\n",
       "      <td>0.351371</td>\n",
       "      <td>0.348506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.126794</td>\n",
       "      <td>0.347074</td>\n",
       "      <td>0.633576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954660</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.804404</td>\n",
       "      <td>-0.141222</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>0.189626</td>\n",
       "      <td>0.571501</td>\n",
       "      <td>0.572042</td>\n",
       "      <td>0.135322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <td>-0.133238</td>\n",
       "      <td>-0.005783</td>\n",
       "      <td>0.130133</td>\n",
       "      <td>0.282248</td>\n",
       "      <td>0.131895</td>\n",
       "      <td>0.135912</td>\n",
       "      <td>0.126794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130461</td>\n",
       "      <td>-0.064222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.095482</td>\n",
       "      <td>0.086842</td>\n",
       "      <td>-0.020204</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.034591</td>\n",
       "      <td>0.068427</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.024774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>-0.999334</td>\n",
       "      <td>-0.327498</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.238779</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.996795</td>\n",
       "      <td>0.347074</td>\n",
       "      <td>0.130461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.268844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349452</td>\n",
       "      <td>0.313730</td>\n",
       "      <td>0.295037</td>\n",
       "      <td>-0.057712</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.082182</td>\n",
       "      <td>0.102453</td>\n",
       "      <td>0.233647</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.053345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_count</th>\n",
       "      <td>0.265178</td>\n",
       "      <td>0.368345</td>\n",
       "      <td>-0.264906</td>\n",
       "      <td>-0.068969</td>\n",
       "      <td>-0.263448</td>\n",
       "      <td>-0.274865</td>\n",
       "      <td>0.633576</td>\n",
       "      <td>-0.064222</td>\n",
       "      <td>-0.268844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660515</td>\n",
       "      <td>0.614204</td>\n",
       "      <td>0.552957</td>\n",
       "      <td>-0.089573</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.068811</td>\n",
       "      <td>0.082580</td>\n",
       "      <td>0.361033</td>\n",
       "      <td>0.361972</td>\n",
       "      <td>0.086132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wps</th>\n",
       "      <td>-0.998677</td>\n",
       "      <td>-0.326850</td>\n",
       "      <td>0.999805</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>0.994415</td>\n",
       "      <td>0.347116</td>\n",
       "      <td>0.129073</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>-0.264693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354194</td>\n",
       "      <td>0.317303</td>\n",
       "      <td>0.298282</td>\n",
       "      <td>-0.057882</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.080743</td>\n",
       "      <td>0.101295</td>\n",
       "      <td>0.234360</td>\n",
       "      <td>0.235376</td>\n",
       "      <td>0.053028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_length</th>\n",
       "      <td>-0.355110</td>\n",
       "      <td>0.337437</td>\n",
       "      <td>0.354533</td>\n",
       "      <td>0.152129</td>\n",
       "      <td>0.355621</td>\n",
       "      <td>0.339885</td>\n",
       "      <td>0.954660</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.349452</td>\n",
       "      <td>0.660515</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907989</td>\n",
       "      <td>0.827676</td>\n",
       "      <td>-0.131156</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.136637</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.547123</td>\n",
       "      <td>0.124357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_no</th>\n",
       "      <td>-0.318744</td>\n",
       "      <td>0.310563</td>\n",
       "      <td>0.317912</td>\n",
       "      <td>0.148471</td>\n",
       "      <td>0.318982</td>\n",
       "      <td>0.306564</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.095482</td>\n",
       "      <td>0.313730</td>\n",
       "      <td>0.614204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.907989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735482</td>\n",
       "      <td>-0.177806</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>0.153252</td>\n",
       "      <td>0.165157</td>\n",
       "      <td>0.532786</td>\n",
       "      <td>0.534185</td>\n",
       "      <td>0.200726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_no</th>\n",
       "      <td>-0.299891</td>\n",
       "      <td>0.283001</td>\n",
       "      <td>0.298891</td>\n",
       "      <td>0.142632</td>\n",
       "      <td>0.299865</td>\n",
       "      <td>0.288581</td>\n",
       "      <td>0.804404</td>\n",
       "      <td>0.086842</td>\n",
       "      <td>0.295037</td>\n",
       "      <td>0.552957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827676</td>\n",
       "      <td>0.735482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035901</td>\n",
       "      <td>0.022856</td>\n",
       "      <td>0.093441</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.431564</td>\n",
       "      <td>0.430991</td>\n",
       "      <td>-0.019837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_deviation</th>\n",
       "      <td>0.057596</td>\n",
       "      <td>-0.066908</td>\n",
       "      <td>-0.057829</td>\n",
       "      <td>-0.024429</td>\n",
       "      <td>-0.057983</td>\n",
       "      <td>-0.057995</td>\n",
       "      <td>-0.141222</td>\n",
       "      <td>-0.020204</td>\n",
       "      <td>-0.057712</td>\n",
       "      <td>-0.089573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131156</td>\n",
       "      <td>-0.177806</td>\n",
       "      <td>-0.035901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086595</td>\n",
       "      <td>-0.093705</td>\n",
       "      <td>-0.053899</td>\n",
       "      <td>-0.105679</td>\n",
       "      <td>-0.107516</td>\n",
       "      <td>-0.471181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_delay</th>\n",
       "      <td>-0.001563</td>\n",
       "      <td>-0.006860</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>-0.000344</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>0.022856</td>\n",
       "      <td>0.086595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024810</td>\n",
       "      <td>-0.030136</td>\n",
       "      <td>-0.041286</td>\n",
       "      <td>-0.042187</td>\n",
       "      <td>-0.112062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_reviews</th>\n",
       "      <td>-0.085232</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.133666</td>\n",
       "      <td>0.083146</td>\n",
       "      <td>0.085654</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>0.048201</td>\n",
       "      <td>0.082182</td>\n",
       "      <td>0.068811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136637</td>\n",
       "      <td>0.153252</td>\n",
       "      <td>0.093441</td>\n",
       "      <td>-0.093705</td>\n",
       "      <td>-0.024810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.489293</td>\n",
       "      <td>0.131325</td>\n",
       "      <td>0.132850</td>\n",
       "      <td>0.068702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewer_days</th>\n",
       "      <td>-0.105965</td>\n",
       "      <td>0.076553</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>0.136980</td>\n",
       "      <td>0.103518</td>\n",
       "      <td>0.105880</td>\n",
       "      <td>0.189626</td>\n",
       "      <td>0.034591</td>\n",
       "      <td>0.102453</td>\n",
       "      <td>0.082580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.165157</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>-0.053899</td>\n",
       "      <td>-0.030136</td>\n",
       "      <td>0.489293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>0.163965</td>\n",
       "      <td>0.058992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stem_sim_length</th>\n",
       "      <td>-0.238218</td>\n",
       "      <td>0.215368</td>\n",
       "      <td>0.235802</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.236302</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.571501</td>\n",
       "      <td>0.068427</td>\n",
       "      <td>0.233647</td>\n",
       "      <td>0.361033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.532786</td>\n",
       "      <td>0.431564</td>\n",
       "      <td>-0.105679</td>\n",
       "      <td>-0.041286</td>\n",
       "      <td>0.131325</td>\n",
       "      <td>0.162446</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995196</td>\n",
       "      <td>0.131599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lem_sim_length</th>\n",
       "      <td>-0.239106</td>\n",
       "      <td>0.215090</td>\n",
       "      <td>0.236770</td>\n",
       "      <td>0.147523</td>\n",
       "      <td>0.237283</td>\n",
       "      <td>0.233856</td>\n",
       "      <td>0.572042</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.234556</td>\n",
       "      <td>0.361972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547123</td>\n",
       "      <td>0.534185</td>\n",
       "      <td>0.430991</td>\n",
       "      <td>-0.107516</td>\n",
       "      <td>-0.042187</td>\n",
       "      <td>0.132850</td>\n",
       "      <td>0.163965</td>\n",
       "      <td>0.995196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>-0.052626</td>\n",
       "      <td>0.064401</td>\n",
       "      <td>0.052886</td>\n",
       "      <td>0.027022</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.054906</td>\n",
       "      <td>0.135322</td>\n",
       "      <td>0.024774</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.086132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124357</td>\n",
       "      <td>0.200726</td>\n",
       "      <td>-0.019837</td>\n",
       "      <td>-0.471181</td>\n",
       "      <td>-0.112062</td>\n",
       "      <td>0.068702</td>\n",
       "      <td>0.058992</td>\n",
       "      <td>0.131599</td>\n",
       "      <td>0.133821</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              flesch_reading_ease  smog_index  \\\n",
       "flesch_reading_ease                      1.000000    0.321961   \n",
       "smog_index                               0.321961    1.000000   \n",
       "flesch_kincaid_grade                    -0.999465   -0.325246   \n",
       "coleman_liau_index                      -0.260971    0.020604   \n",
       "automated_readability_index             -0.999300   -0.323973   \n",
       "dale_chall_readability_score            -0.996831   -0.324201   \n",
       "difficult_words                         -0.355395    0.363657   \n",
       "linsear_write_formula                   -0.133238   -0.005783   \n",
       "gunning_fog                             -0.999334   -0.327498   \n",
       "sentence_count                           0.265178    0.368345   \n",
       "wps                                     -0.998677   -0.326850   \n",
       "review_length                           -0.355110    0.337437   \n",
       "pos_no                                  -0.318744    0.310563   \n",
       "neg_no                                  -0.299891    0.283001   \n",
       "user_deviation                           0.057596   -0.066908   \n",
       "user_delay                              -0.001563   -0.006860   \n",
       "no_of_reviews                           -0.085232    0.066433   \n",
       "reviewer_days                           -0.105965    0.076553   \n",
       "stem_sim_length                         -0.238218    0.215368   \n",
       "lem_sim_length                          -0.239106    0.215090   \n",
       "overall                                 -0.052626    0.064401   \n",
       "\n",
       "                              flesch_kincaid_grade  coleman_liau_index  \\\n",
       "flesch_reading_ease                      -0.999465           -0.260971   \n",
       "smog_index                               -0.325246            0.020604   \n",
       "flesch_kincaid_grade                      1.000000            0.236112   \n",
       "coleman_liau_index                        0.236112            1.000000   \n",
       "automated_readability_index               0.999913            0.241079   \n",
       "dale_chall_readability_score              0.995617            0.281476   \n",
       "difficult_words                           0.350158            0.280264   \n",
       "linsear_write_formula                     0.130133            0.282248   \n",
       "gunning_fog                               0.999815            0.238779   \n",
       "sentence_count                           -0.264906           -0.068969   \n",
       "wps                                       0.999805            0.222411   \n",
       "review_length                             0.354533            0.152129   \n",
       "pos_no                                    0.317912            0.148471   \n",
       "neg_no                                    0.298891            0.142632   \n",
       "user_deviation                           -0.057829           -0.024429   \n",
       "user_delay                                0.001863           -0.008178   \n",
       "no_of_reviews                             0.082451            0.133666   \n",
       "reviewer_days                             0.103036            0.136980   \n",
       "stem_sim_length                           0.235802            0.148900   \n",
       "lem_sim_length                            0.236770            0.147523   \n",
       "overall                                   0.052886            0.027022   \n",
       "\n",
       "                              automated_readability_index  \\\n",
       "flesch_reading_ease                             -0.999300   \n",
       "smog_index                                      -0.323973   \n",
       "flesch_kincaid_grade                             0.999913   \n",
       "coleman_liau_index                               0.241079   \n",
       "automated_readability_index                      1.000000   \n",
       "dale_chall_readability_score                     0.995590   \n",
       "difficult_words                                  0.351371   \n",
       "linsear_write_formula                            0.131895   \n",
       "gunning_fog                                      0.999780   \n",
       "sentence_count                                  -0.263448   \n",
       "wps                                              0.999766   \n",
       "review_length                                    0.355621   \n",
       "pos_no                                           0.318982   \n",
       "neg_no                                           0.299865   \n",
       "user_deviation                                  -0.057983   \n",
       "user_delay                                       0.001912   \n",
       "no_of_reviews                                    0.083146   \n",
       "reviewer_days                                    0.103518   \n",
       "stem_sim_length                                  0.236302   \n",
       "lem_sim_length                                   0.237283   \n",
       "overall                                          0.053202   \n",
       "\n",
       "                              dale_chall_readability_score  difficult_words  \\\n",
       "flesch_reading_ease                              -0.996831        -0.355395   \n",
       "smog_index                                       -0.324201         0.363657   \n",
       "flesch_kincaid_grade                              0.995617         0.350158   \n",
       "coleman_liau_index                                0.281476         0.280264   \n",
       "automated_readability_index                       0.995590         0.351371   \n",
       "dale_chall_readability_score                      1.000000         0.348506   \n",
       "difficult_words                                   0.348506         1.000000   \n",
       "linsear_write_formula                             0.135912         0.126794   \n",
       "gunning_fog                                       0.996795         0.347074   \n",
       "sentence_count                                   -0.274865         0.633576   \n",
       "wps                                               0.994415         0.347116   \n",
       "review_length                                     0.339885         0.954660   \n",
       "pos_no                                            0.306564         0.884000   \n",
       "neg_no                                            0.288581         0.804404   \n",
       "user_deviation                                   -0.057995        -0.141222   \n",
       "user_delay                                       -0.000344        -0.002968   \n",
       "no_of_reviews                                     0.085654         0.167861   \n",
       "reviewer_days                                     0.105880         0.189626   \n",
       "stem_sim_length                                   0.233200         0.571501   \n",
       "lem_sim_length                                    0.233856         0.572042   \n",
       "overall                                           0.054906         0.135322   \n",
       "\n",
       "                              linsear_write_formula  gunning_fog  \\\n",
       "flesch_reading_ease                       -0.133238    -0.999334   \n",
       "smog_index                                -0.005783    -0.327498   \n",
       "flesch_kincaid_grade                       0.130133     0.999815   \n",
       "coleman_liau_index                         0.282248     0.238779   \n",
       "automated_readability_index                0.131895     0.999780   \n",
       "dale_chall_readability_score               0.135912     0.996795   \n",
       "difficult_words                            0.126794     0.347074   \n",
       "linsear_write_formula                      1.000000     0.130461   \n",
       "gunning_fog                                0.130461     1.000000   \n",
       "sentence_count                            -0.064222    -0.268844   \n",
       "wps                                        0.129073     0.999580   \n",
       "review_length                              0.100037     0.349452   \n",
       "pos_no                                     0.095482     0.313730   \n",
       "neg_no                                     0.086842     0.295037   \n",
       "user_deviation                            -0.020204    -0.057712   \n",
       "user_delay                                -0.007936     0.001378   \n",
       "no_of_reviews                              0.048201     0.082182   \n",
       "reviewer_days                              0.034591     0.102453   \n",
       "stem_sim_length                            0.068427     0.233647   \n",
       "lem_sim_length                             0.068935     0.234556   \n",
       "overall                                    0.024774     0.053345   \n",
       "\n",
       "                              sentence_count    ...     review_length  \\\n",
       "flesch_reading_ease                 0.265178    ...         -0.355110   \n",
       "smog_index                          0.368345    ...          0.337437   \n",
       "flesch_kincaid_grade               -0.264906    ...          0.354533   \n",
       "coleman_liau_index                 -0.068969    ...          0.152129   \n",
       "automated_readability_index        -0.263448    ...          0.355621   \n",
       "dale_chall_readability_score       -0.274865    ...          0.339885   \n",
       "difficult_words                     0.633576    ...          0.954660   \n",
       "linsear_write_formula              -0.064222    ...          0.100037   \n",
       "gunning_fog                        -0.268844    ...          0.349452   \n",
       "sentence_count                      1.000000    ...          0.660515   \n",
       "wps                                -0.264693    ...          0.354194   \n",
       "review_length                       0.660515    ...          1.000000   \n",
       "pos_no                              0.614204    ...          0.907989   \n",
       "neg_no                              0.552957    ...          0.827676   \n",
       "user_deviation                     -0.089573    ...         -0.131156   \n",
       "user_delay                          0.004338    ...          0.004538   \n",
       "no_of_reviews                       0.068811    ...          0.136637   \n",
       "reviewer_days                       0.082580    ...          0.160465   \n",
       "stem_sim_length                     0.361033    ...          0.545151   \n",
       "lem_sim_length                      0.361972    ...          0.547123   \n",
       "overall                             0.086132    ...          0.124357   \n",
       "\n",
       "                                pos_no    neg_no  user_deviation  user_delay  \\\n",
       "flesch_reading_ease          -0.318744 -0.299891        0.057596   -0.001563   \n",
       "smog_index                    0.310563  0.283001       -0.066908   -0.006860   \n",
       "flesch_kincaid_grade          0.317912  0.298891       -0.057829    0.001863   \n",
       "coleman_liau_index            0.148471  0.142632       -0.024429   -0.008178   \n",
       "automated_readability_index   0.318982  0.299865       -0.057983    0.001912   \n",
       "dale_chall_readability_score  0.306564  0.288581       -0.057995   -0.000344   \n",
       "difficult_words               0.884000  0.804404       -0.141222   -0.002968   \n",
       "linsear_write_formula         0.095482  0.086842       -0.020204   -0.007936   \n",
       "gunning_fog                   0.313730  0.295037       -0.057712    0.001378   \n",
       "sentence_count                0.614204  0.552957       -0.089573    0.004338   \n",
       "wps                           0.317303  0.298282       -0.057882    0.002023   \n",
       "review_length                 0.907989  0.827676       -0.131156    0.004538   \n",
       "pos_no                        1.000000  0.735482       -0.177806   -0.005693   \n",
       "neg_no                        0.735482  1.000000       -0.035901    0.022856   \n",
       "user_deviation               -0.177806 -0.035901        1.000000    0.086595   \n",
       "user_delay                   -0.005693  0.022856        0.086595    1.000000   \n",
       "no_of_reviews                 0.153252  0.093441       -0.093705   -0.024810   \n",
       "reviewer_days                 0.165157  0.126494       -0.053899   -0.030136   \n",
       "stem_sim_length               0.532786  0.431564       -0.105679   -0.041286   \n",
       "lem_sim_length                0.534185  0.430991       -0.107516   -0.042187   \n",
       "overall                       0.200726 -0.019837       -0.471181   -0.112062   \n",
       "\n",
       "                              no_of_reviews  reviewer_days  stem_sim_length  \\\n",
       "flesch_reading_ease               -0.085232      -0.105965        -0.238218   \n",
       "smog_index                         0.066433       0.076553         0.215368   \n",
       "flesch_kincaid_grade               0.082451       0.103036         0.235802   \n",
       "coleman_liau_index                 0.133666       0.136980         0.148900   \n",
       "automated_readability_index        0.083146       0.103518         0.236302   \n",
       "dale_chall_readability_score       0.085654       0.105880         0.233200   \n",
       "difficult_words                    0.167861       0.189626         0.571501   \n",
       "linsear_write_formula              0.048201       0.034591         0.068427   \n",
       "gunning_fog                        0.082182       0.102453         0.233647   \n",
       "sentence_count                     0.068811       0.082580         0.361033   \n",
       "wps                                0.080743       0.101295         0.234360   \n",
       "review_length                      0.136637       0.160465         0.545151   \n",
       "pos_no                             0.153252       0.165157         0.532786   \n",
       "neg_no                             0.093441       0.126494         0.431564   \n",
       "user_deviation                    -0.093705      -0.053899        -0.105679   \n",
       "user_delay                        -0.024810      -0.030136        -0.041286   \n",
       "no_of_reviews                      1.000000       0.489293         0.131325   \n",
       "reviewer_days                      0.489293       1.000000         0.162446   \n",
       "stem_sim_length                    0.131325       0.162446         1.000000   \n",
       "lem_sim_length                     0.132850       0.163965         0.995196   \n",
       "overall                            0.068702       0.058992         0.131599   \n",
       "\n",
       "                              lem_sim_length   overall  \n",
       "flesch_reading_ease                -0.239106 -0.052626  \n",
       "smog_index                          0.215090  0.064401  \n",
       "flesch_kincaid_grade                0.236770  0.052886  \n",
       "coleman_liau_index                  0.147523  0.027022  \n",
       "automated_readability_index         0.237283  0.053202  \n",
       "dale_chall_readability_score        0.233856  0.054906  \n",
       "difficult_words                     0.572042  0.135322  \n",
       "linsear_write_formula               0.068935  0.024774  \n",
       "gunning_fog                         0.234556  0.053345  \n",
       "sentence_count                      0.361972  0.086132  \n",
       "wps                                 0.235376  0.053028  \n",
       "review_length                       0.547123  0.124357  \n",
       "pos_no                              0.534185  0.200726  \n",
       "neg_no                              0.430991 -0.019837  \n",
       "user_deviation                     -0.107516 -0.471181  \n",
       "user_delay                         -0.042187 -0.112062  \n",
       "no_of_reviews                       0.132850  0.068702  \n",
       "reviewer_days                       0.163965  0.058992  \n",
       "stem_sim_length                     0.995196  0.131599  \n",
       "lem_sim_length                      1.000000  0.133821  \n",
       "overall                             0.133821  1.000000  \n",
       "\n",
       "[21 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJYAAALKCAYAAABgC9vFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x220027d8390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ht=sns.heatmap(X.corr(),cmap='coolwarm', linewidths=.5,annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig.savefig('correlation_Final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
       "       'overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = X.columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_correlation_df = pd.DataFrame(columns=['Feature_1', 'Feature_2', 'Correlation'])\n",
    "threshold = 0.8\n",
    "for i in range(0,feature_size):\n",
    "    f1 = features[i]\n",
    "    for j in range(0,feature_size):\n",
    "        if j > i :\n",
    "            correlation = x_corr.iloc[i].iloc[j]\n",
    "            f2 = features[j]\n",
    "            if abs(correlation) >= threshold :\n",
    "                corr_series = pd.Series({'Feature_1':f1,'Feature_2':f2,'Correlation':correlation})\n",
    "                top_correlation_df=top_correlation_df.append(corr_series, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_correlation_df.index = top_correlation_df.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>-0.999465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>-0.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>-0.996831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>-0.999334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>wps</td>\n",
       "      <td>-0.998677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>0.999913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>0.995617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>0.999815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>wps</td>\n",
       "      <td>0.999805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>0.995590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>0.999780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>wps</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>0.996795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>wps</td>\n",
       "      <td>0.994415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>review_length</td>\n",
       "      <td>0.954660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>pos_no</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>neg_no</td>\n",
       "      <td>0.804404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>wps</td>\n",
       "      <td>0.999580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_length</td>\n",
       "      <td>pos_no</td>\n",
       "      <td>0.907989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_length</td>\n",
       "      <td>neg_no</td>\n",
       "      <td>0.827676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stem_sim_length</td>\n",
       "      <td>lem_sim_length</td>\n",
       "      <td>0.995196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature_1                     Feature_2  Correlation\n",
       "1            flesch_reading_ease          flesch_kincaid_grade    -0.999465\n",
       "2            flesch_reading_ease   automated_readability_index    -0.999300\n",
       "3            flesch_reading_ease  dale_chall_readability_score    -0.996831\n",
       "4            flesch_reading_ease                   gunning_fog    -0.999334\n",
       "5            flesch_reading_ease                           wps    -0.998677\n",
       "6           flesch_kincaid_grade   automated_readability_index     0.999913\n",
       "7           flesch_kincaid_grade  dale_chall_readability_score     0.995617\n",
       "8           flesch_kincaid_grade                   gunning_fog     0.999815\n",
       "9           flesch_kincaid_grade                           wps     0.999805\n",
       "10   automated_readability_index  dale_chall_readability_score     0.995590\n",
       "11   automated_readability_index                   gunning_fog     0.999780\n",
       "12   automated_readability_index                           wps     0.999766\n",
       "13  dale_chall_readability_score                   gunning_fog     0.996795\n",
       "14  dale_chall_readability_score                           wps     0.994415\n",
       "15               difficult_words                 review_length     0.954660\n",
       "16               difficult_words                        pos_no     0.884000\n",
       "17               difficult_words                        neg_no     0.804404\n",
       "18                   gunning_fog                           wps     0.999580\n",
       "19                 review_length                        pos_no     0.907989\n",
       "20                 review_length                        neg_no     0.827676\n",
       "21               stem_sim_length                lem_sim_length     0.995196"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_correlation_df.to_csv('Top_Correlation_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['smog_index', 'wps', 'coleman_liau_index', 'linsear_write_formula',\n",
      "       'sentence_count', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
      "       'no_of_reviews', 'reviewer_days', 'lem_sim_length', 'overall'],\n",
      "      dtype='object')\n",
      "No of Features:  13\n"
     ]
    }
   ],
   "source": [
    "X = text_df[[ 'smog_index', 'wps', 'coleman_liau_index',\n",
    "       'linsear_write_formula', 'sentence_count', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['lem_sim_length','overall']])\n",
    "\n",
    "print(X.columns)\n",
    "print(\"No of Features: \",len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smog_index</th>\n",
       "      <th>wps</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>7.69</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>9.72</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.3</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.8</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>8.05</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>8.09</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   smog_index         wps  coleman_liau_index  linsear_write_formula  \\\n",
       "0         0.0  427.000000                7.69              14.000000   \n",
       "1         0.0  423.000000                9.72               8.666667   \n",
       "2         7.3   19.521739                5.92               7.666667   \n",
       "3        10.8   12.800000                8.05               7.200000   \n",
       "4         0.0  138.000000                8.09               4.400000   \n",
       "\n",
       "   sentence_count  pos_no  neg_no  user_deviation  user_delay  no_of_reviews  \\\n",
       "0               1       2       4        1.399189  10152000.0              1   \n",
       "1               2      18       8        0.624437  10886400.0              1   \n",
       "2              23       7       6        0.510375   7516800.0              1   \n",
       "3               5       2       0        0.442171   2678400.0              1   \n",
       "4               1       1       3        1.616074   7603200.0              1   \n",
       "\n",
       "   reviewer_days  lem_sim_length  overall  \n",
       "0          260.0              13      1.0  \n",
       "1         1390.0              23      3.0  \n",
       "2          929.0              16      2.0  \n",
       "3            0.0               2      5.0  \n",
       "4            0.0               2      1.0  "
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate : 0.09 , Best No of Estimators : 400 , Best Subsample size : 0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03749584249070656\n",
      "Root Mean Squared Error (RMSE): 0.19363843236998837\n",
      "Mean Absolute Error (MAE):      0.1335894220636948\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2033476304568681, 'mae': 0.1572598743815295}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.20359316383768933, 'mae': 0.1574722327778122}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20379005892453583, 'mae': 0.1576244288572968}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20384708183632821, 'mae': 0.1576827554095967}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19731976332700435, 'mae': 0.1423085072352256}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.197575500529425, 'mae': 0.1425071965580442}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19775580404169904, 'mae': 0.14270564787289575}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1978348977067556, 'mae': 0.1427958165350609}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1956771485201474, 'mae': 0.13729316639526135}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19586470374070525, 'mae': 0.1374758143332403}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19603721963320278, 'mae': 0.1377165520031102}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19608980960289268, 'mae': 0.13778059507540322}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19486481575881834, 'mae': 0.13526430754764865}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19503505418127803, 'mae': 0.13548178747684225}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19520922161356707, 'mae': 0.13576773947989232}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.195199332994647, 'mae': 0.1358090417421177}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19372037298622882, 'mae': 0.13314969902168192}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19308190160068356, 'mae': 0.13284605154604504}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.1928807448973994, 'mae': 0.13284563454523426}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19298229859472144, 'mae': 0.1329916768445648}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19347952468768315, 'mae': 0.1332013182964349}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19278580526699968, 'mae': 0.13270512341965937}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19249531286871532, 'mae': 0.13249347276691012}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19276480021724746, 'mae': 0.1327318820614529}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19356019562349042, 'mae': 0.13303414383598924}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19276980150587172, 'mae': 0.13245828824183795}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1924226773642787, 'mae': 0.13235961470501104}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19263958154867442, 'mae': 0.13254179221722254}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19352857583113084, 'mae': 0.13276868377469225}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.1927765249488596, 'mae': 0.1323065913673192}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19248574787674927, 'mae': 0.13233826751275282}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19261361519972825, 'mae': 0.1324659045611545}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19356776703197212, 'mae': 0.13315049719948735}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19311018899656054, 'mae': 0.13300109427990853}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19290027050109282, 'mae': 0.13278974706984123}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19300596683175042, 'mae': 0.1329161101635913}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19355229764766146, 'mae': 0.13325241368860002}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19305541859718056, 'mae': 0.13297572762381024}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19264415782084696, 'mae': 0.13249034437095833}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19282652365119485, 'mae': 0.13265042327624355}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19370651791419338, 'mae': 0.133150572682603}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19300966857868818, 'mae': 0.13271258121197996}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19253349828547284, 'mae': 0.13232935886336883}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19282879247966594, 'mae': 0.13259208469073797}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19377967943652147, 'mae': 0.13296600631934544}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19308740298331128, 'mae': 0.1325451288352066}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19260675275208616, 'mae': 0.13233504050893463}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19287312686349775, 'mae': 0.1326021838578727}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19490198979261836, 'mae': 0.13344886915822377}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19315784989689527, 'mae': 0.13258288609525182}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19284811847321917, 'mae': 0.13256270390521913}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19298362539634614, 'mae': 0.13257898034397805}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.1950903748361387, 'mae': 0.13393454448477587}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19351175471939794, 'mae': 0.13297716217689198}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19305182817214858, 'mae': 0.13259313551002455}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1929888801538933, 'mae': 0.13253040772407165}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19545285934886705, 'mae': 0.13396288443795734}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19391504504005264, 'mae': 0.13304861836600781}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1930531623713298, 'mae': 0.13250573693340537}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19310003086357377, 'mae': 0.1325115949308227}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19596016794208926, 'mae': 0.13409957702817404}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.1943492184898216, 'mae': 0.13328035193899151}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19328736203425084, 'mae': 0.1326413163293455}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1932654176645484, 'mae': 0.13261553137783552}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20136162962040818, 'mae': 0.13702252136189716}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19818468232110933, 'mae': 0.1354104803451944}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19525429850384937, 'mae': 0.13395937522842616}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1949535178421295, 'mae': 0.13370970667110602}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.2033848304305754, 'mae': 0.13910441537428098}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.2002498975312421, 'mae': 0.13688802592159313}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19633519213485467, 'mae': 0.1347799536623886}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19617901645656782, 'mae': 0.13456094919922265}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.20509419924846434, 'mae': 0.13966607321661048}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.20217281475323157, 'mae': 0.13804061153772998}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19758024304207195, 'mae': 0.13561853897917087}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1970559141182705, 'mae': 0.13511978388851958}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.20826491022709573, 'mae': 0.14146953674327634}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.2033274076217017, 'mae': 0.1390163278452075}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19849996284231314, 'mae': 0.13617630022925095}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19787039415939506, 'mae': 0.1358311869530384}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.21830751191145326, 'mae': 0.14494559315411557}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.20708804378694254, 'mae': 0.1400037262466946}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20127396779006926, 'mae': 0.13738096405742142}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19983831095109109, 'mae': 0.13704368927079735}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.22912779136994557, 'mae': 0.1494528912347786}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.2117153164173738, 'mae': 0.14372064993401834}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.20403566954896, 'mae': 0.13931775732110785}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.20165629327896006, 'mae': 0.13812458920218168}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.23515675716874151, 'mae': 0.1519936811817088}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.21561537672874692, 'mae': 0.1458960551453729}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.20604349997192528, 'mae': 0.14088377378646402}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.20379421637022405, 'mae': 0.13986705457788876}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.24469771551024105, 'mae': 0.15604615942008926}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.218706170754104, 'mae': 0.14829135956659248}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.20795252962183022, 'mae': 0.1425180844686298}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.206062069933791, 'mae': 0.14156641921510824}\n"
     ]
    }
   ],
   "source": [
    "best_rmse_4=1000\n",
    "best_mae_4=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_4) :\n",
    "                if(mae < best_mae_4) :\n",
    "                    best_rmse_4 = rmse\n",
    "                    best_mae_4 = mae\n",
    "                    best_learning_rate_4 = rate\n",
    "                    best_n_estimators_4 = estimator\n",
    "                    best_subsample_4 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13235961470501104"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mae_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1924226773642787"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Feature Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
      "       'coleman_liau_index', 'automated_readability_index',\n",
      "       'dale_chall_readability_score', 'difficult_words',\n",
      "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
      "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
      "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
      "       'overall'],\n",
      "      dtype='object')\n",
      "No of Features:  21\n"
     ]
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "print(\"Features: \",X.columns)\n",
    "print(\"No of Features: \",len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result= pd.DataFrame(columns=['feature','correlation','rmse-LR','mae-LR','rmse-RF','mae-RF','rmse-XGB','mae-XGB'])\n",
    "for col in X.columns:\n",
    "    tempdict={}\n",
    "    tempdict['feature']=col\n",
    "    # GET CORRELATION WITH HELPFULNESS\n",
    "    a = X[col]\n",
    "    tempdict['correlation'] = a.corr(y)\n",
    "    \n",
    "    train = X_train[col]\n",
    "    test  = X_test[col]\n",
    "    \n",
    "    train = np.array(train).reshape(-1,1)\n",
    "    test = np.array(test).reshape(-1,1)\n",
    "   \n",
    "    #LINEAR REGRESSION\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(train,y_train)\n",
    "    predictions = lm.predict(test)\n",
    "    \n",
    "    tempdict['rmse-LR']=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "    tempdict['mae-LR']=mean_absolute_error(y_test,predictions)\n",
    "    \n",
    "    #RANDOM FOREST\n",
    "    regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "    regressor.fit(train,y_train)\n",
    "    preds_2 = regressor.predict(test)\n",
    "\n",
    "    tempdict['rmse-RF']=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "    tempdict['mae-RF']=mean_absolute_error(y_test,preds_2)\n",
    "    \n",
    "    #XGBOOST\n",
    "    xgb_reg = xgb.XGBRegressor()\n",
    "    xgb_reg.fit(train,y_train)\n",
    "    preds_3 = xgb_reg.predict(test)\n",
    "    \n",
    "    tempdict['rmse-XGB'] = math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "    tempdict['mae-XGB'] = mean_absolute_error(y_test,preds_3)\n",
    "    \n",
    "    \n",
    "    tempSeries=pd.Series(tempdict)\n",
    "    featurewise_result=featurewise_result.append(tempSeries, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result.to_csv('Featurewise_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category-wise Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorywise_result= pd.DataFrame(columns=['category','rmse-LR','mae-LR','rmse-RF','mae-RF', 'rmse-XGB', 'mae-XGB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#USER FEATURES vs. HELPFULNESS\n",
    "tempdict={}\n",
    "tempdict['category']='User Features'\n",
    "\n",
    "cols = ['user_deviation', 'user_delay', 'no_of_reviews', 'reviewer_days']\n",
    "train = X_train[cols]\n",
    "test  = X_test[cols]\n",
    "\n",
    "#LINEAR REGRESSION\n",
    "lm = LinearRegression()\n",
    "lm.fit(train,y_train)\n",
    "predictions = lm.predict(test)\n",
    "\n",
    "\n",
    "tempdict['rmse-LR']=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "tempdict['mae-LR']=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "#RANDOM FOREST\n",
    "regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "regressor.fit(train,y_train)\n",
    "preds_2 = regressor.predict(test)\n",
    "\n",
    "tempdict['rmse-RF']=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "tempdict['mae-RF']=mean_absolute_error(y_test,preds_2)\n",
    "\n",
    "#XGBOOST\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(train,y_train)\n",
    "preds_3 = xgb_reg.predict(test)\n",
    "\n",
    "tempdict['rmse-XGB'] = math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "tempdict['mae-XGB'] = mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "categorywise_result=categorywise_result.append(tempSeries, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rmse-LR</th>\n",
       "      <th>mae-LR</th>\n",
       "      <th>rmse-RF</th>\n",
       "      <th>mae-RF</th>\n",
       "      <th>rmse-XGB</th>\n",
       "      <th>mae-XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User Features</td>\n",
       "      <td>0.223521</td>\n",
       "      <td>0.159481</td>\n",
       "      <td>0.236387</td>\n",
       "      <td>0.165092</td>\n",
       "      <td>0.221966</td>\n",
       "      <td>0.158669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category   rmse-LR    mae-LR   rmse-RF    mae-RF  rmse-XGB   mae-XGB\n",
       "0  User Features  0.223521  0.159481  0.236387  0.165092  0.221966  0.158669"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorywise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REVIEW TEXT FEATURES vs. HELPFULNESS\n",
    "tempdict={}\n",
    "tempdict['category']='Review Text Features'\n",
    "\n",
    "cols = ['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
    "       'coleman_liau_index', 'automated_readability_index',\n",
    "       'dale_chall_readability_score', 'difficult_words',\n",
    "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
    "       'review_length', 'pos_no', 'neg_no',]\n",
    "train = X_train[cols]\n",
    "test  = X_test[cols]\n",
    "\n",
    "#LINEAR REGRESSION\n",
    "lm = LinearRegression()\n",
    "lm.fit(train,y_train)\n",
    "predictions = lm.predict(test)\n",
    "\n",
    "\n",
    "tempdict['rmse-LR']=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "tempdict['mae-LR']=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "#RANDOM FOREST\n",
    "regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "regressor.fit(train,y_train)\n",
    "preds_2 = regressor.predict(test)\n",
    "\n",
    "\n",
    "tempdict['rmse-RF']=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "tempdict['mae-RF']=mean_absolute_error(y_test,preds_2)\n",
    "\n",
    "#XGBOOST\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(train,y_train)\n",
    "preds_3 = xgb_reg.predict(test)\n",
    "\n",
    "tempdict['rmse-XGB'] = math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "tempdict['mae-XGB'] = mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "categorywise_result=categorywise_result.append(tempSeries, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#METADATA FEATURES vs. HELPFULNESS\n",
    "tempdict={}\n",
    "tempdict['category']='Meta Data Features'\n",
    "\n",
    "cols = ['stem_sim_length', 'lem_sim_length', 'overall']\n",
    "train = X_train[cols]\n",
    "test  = X_test[cols]\n",
    "\n",
    "#LINEAR REGRESSION\n",
    "lm = LinearRegression()\n",
    "lm.fit(train,y_train)\n",
    "predictions = lm.predict(test)\n",
    "\n",
    "\n",
    "tempdict['rmse-LR']=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "tempdict['mae-LR']=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "#RANDOM FOREST\n",
    "regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "regressor.fit(train,y_train)\n",
    "preds_2 = regressor.predict(test)\n",
    "\n",
    "\n",
    "tempdict['rmse-RF']=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "tempdict['mae-RF']=mean_absolute_error(y_test,preds_2)\n",
    "\n",
    "#XGBOOST\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(train,y_train)\n",
    "preds_3 = xgb_reg.predict(test)\n",
    "\n",
    "tempdict['rmse-XGB'] = math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "tempdict['mae-XGB'] = mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "categorywise_result=categorywise_result.append(tempSeries, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorywise_result.to_csv('Categorywise_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rmse-LR</th>\n",
       "      <th>mae-LR</th>\n",
       "      <th>rmse-RF</th>\n",
       "      <th>mae-RF</th>\n",
       "      <th>rmse-XGB</th>\n",
       "      <th>mae-XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User Features</td>\n",
       "      <td>0.223521</td>\n",
       "      <td>0.159481</td>\n",
       "      <td>0.236387</td>\n",
       "      <td>0.165092</td>\n",
       "      <td>0.221966</td>\n",
       "      <td>0.158669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Text Features</td>\n",
       "      <td>0.225886</td>\n",
       "      <td>0.165162</td>\n",
       "      <td>0.223141</td>\n",
       "      <td>0.162692</td>\n",
       "      <td>0.217774</td>\n",
       "      <td>0.156143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meta Data Features</td>\n",
       "      <td>0.208531</td>\n",
       "      <td>0.145716</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>0.142711</td>\n",
       "      <td>0.204963</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category   rmse-LR    mae-LR   rmse-RF    mae-RF  rmse-XGB  \\\n",
       "0         User Features  0.223521  0.159481  0.236387  0.165092  0.221966   \n",
       "1  Review Text Features  0.225886  0.165162  0.223141  0.162692  0.217774   \n",
       "2    Meta Data Features  0.208531  0.145716  0.206249  0.142711  0.204963   \n",
       "\n",
       "    mae-XGB  \n",
       "0  0.158669  \n",
       "1  0.156143  \n",
       "2  0.141700  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorywise_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-wise Prediction - All Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelwise_result = pd.DataFrame(columns=['Prediction Model', 'RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_learning_rate=0.09\n",
    "grid_n_estimators = 500\n",
    "grid_subsample = 0.9\n",
    "\n",
    "tempdict={}\n",
    "\n",
    "train = X_train\n",
    "test  = X_test\n",
    "\n",
    "#LINEAR REGRESSION\n",
    "tempdict['Prediction Model']='Linear Regression'\n",
    "lm = LinearRegression()\n",
    "lm.fit(train,y_train)\n",
    "predictions = lm.predict(test)\n",
    "\n",
    "\n",
    "tempdict['RMSE']=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "tempdict['MAE']=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "modelwise_result=modelwise_result.append(tempSeries, ignore_index=True)\n",
    "\n",
    "#RANDOM FOREST\n",
    "tempdict['Prediction Model']='Random Forest'\n",
    "regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "regressor.fit(train,y_train)\n",
    "preds_2 = regressor.predict(test)\n",
    "\n",
    "\n",
    "tempdict['RMSE']=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "tempdict['MAE']=mean_absolute_error(y_test,preds_2)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "modelwise_result=modelwise_result.append(tempSeries, ignore_index=True)\n",
    "\n",
    "#XGBOOST\n",
    "tempdict['Prediction Model']='Extreme Gradient Boosting'\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(train,y_train)\n",
    "preds_3 = xgb_reg.predict(test)\n",
    "\n",
    "tempdict['RMSE']=math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "tempdict['MAE']=mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "modelwise_result=modelwise_result.append(tempSeries, ignore_index=True)\n",
    "\n",
    "#XGBOOST WITH GRID SEARCH\n",
    "tempdict['Prediction Model']='XGBoost with Grid Search'\n",
    "xgb_reg = xgb.XGBRegressor(learning_rate=grid_learning_rate, n_estimators=grid_n_estimators, subsample=grid_subsample)\n",
    "xgb_reg.fit(train,y_train)\n",
    "preds_3 = xgb_reg.predict(test)\n",
    "\n",
    "tempdict['RMSE']=math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "tempdict['MAE']=mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "tempSeries=pd.Series(tempdict)\n",
    "modelwise_result=modelwise_result.append(tempSeries, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelwise_result.to_csv('All_Feature_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.201030</td>\n",
       "      <td>0.141376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.194449</td>\n",
       "      <td>0.136248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extreme Gradient Boosting</td>\n",
       "      <td>0.192803</td>\n",
       "      <td>0.132932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost with Grid Search</td>\n",
       "      <td>0.192087</td>\n",
       "      <td>0.131920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Prediction Model      RMSE       MAE\n",
       "0          Linear Regression  0.201030  0.141376\n",
       "1              Random Forest  0.194449  0.136248\n",
       "2  Extreme Gradient Boosting  0.192803  0.132932\n",
       "3   XGBoost with Grid Search  0.192087  0.131920"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelwise_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Helper Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
       "       'coleman_liau_index', 'automated_readability_index',\n",
       "       'dale_chall_readability_score', 'difficult_words',\n",
       "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
       "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
       "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
       "       'overall', 'help_pred_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "X = X.join(helper_df['help_pred_2'])\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "      <th>help_pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count     ...       pos_no  \\\n",
       "0              14.000000       176.92               1     ...            2   \n",
       "1               8.666667       175.64               2     ...           18   \n",
       "2               7.666667        13.01              23     ...            7   \n",
       "3               7.200000        14.00               5     ...            2   \n",
       "4               4.400000        61.55               1     ...            1   \n",
       "\n",
       "   neg_no  user_deviation  user_delay  no_of_reviews  reviewer_days  \\\n",
       "0       4        1.399189  10152000.0              1          260.0   \n",
       "1       8        0.624437  10886400.0              1         1390.0   \n",
       "2       6        0.510375   7516800.0              1          929.0   \n",
       "3       0        0.442171   2678400.0              1            0.0   \n",
       "4       3        1.616074   7603200.0              1            0.0   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  help_pred_2  \n",
       "0               15              13      1.0            0  \n",
       "1               24              23      3.0            1  \n",
       "2               17              16      2.0            0  \n",
       "3                2               2      5.0            1  \n",
       "4                2               2      1.0            0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03728177323603432\n",
      "Root Mean Squared Error (RMSE): 0.19308488608908342\n",
      "Mean Absolute Error (MAE):      0.13320756116264879\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.01,0.09,0.1,0.2,0.5,0.9], \n",
    "    'n_estimators': [200,300,400,500], \n",
    "    'subsample': [0.3, 0.5, 0.9, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['learning_rate','n_estimators', 'subsample', 'rmse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20139545942771958, 'mae': 0.15560848916060288}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.20160953301534293, 'mae': 0.1557887123412146}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20182420616036403, 'mae': 0.1559246233298197}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20180779338027474, 'mae': 0.15590294308081656}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19558117048017537, 'mae': 0.14089706748998346}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1957829190617543, 'mae': 0.14104662132640136}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19598129759494692, 'mae': 0.1412405058350879}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19602729681738457, 'mae': 0.1412806723727846}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19426957396304234, 'mae': 0.13619446623924683}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.1943886915308246, 'mae': 0.13631924072299412}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19457117687051423, 'mae': 0.13655340395379048}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19464927153551156, 'mae': 0.13665068615023546}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.193799422858233, 'mae': 0.13444571743692454}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.1938278597685786, 'mae': 0.13454511542342404}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19398615937110908, 'mae': 0.13482011312383696}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19401410706769875, 'mae': 0.13490990792534158}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19324581405469227, 'mae': 0.13290807040917021}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19287360323209415, 'mae': 0.13270814493391367}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.1926973798067669, 'mae': 0.13271885376684614}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19252977787279357, 'mae': 0.1326768137924307}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19304227587304568, 'mae': 0.13293544822605655}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19283731004240082, 'mae': 0.13268560160671755}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19237566423586933, 'mae': 0.13236765547657192}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19232580221711262, 'mae': 0.13243803282297564}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1932203775471882, 'mae': 0.13281068207473745}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19283033558875254, 'mae': 0.13242060483259016}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19223114620400353, 'mae': 0.13215340062000652}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1923299966275741, 'mae': 0.13239665287981406}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.1933452992107152, 'mae': 0.13256877431753564}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19288893563235013, 'mae': 0.13231708741734502}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19228895614724076, 'mae': 0.13211752727510595}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1923005091741656, 'mae': 0.1323170345298934}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19315592627144032, 'mae': 0.13282931823659164}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19279988495642256, 'mae': 0.13272951542078767}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19258009447203012, 'mae': 0.13264739160385586}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1925611043902423, 'mae': 0.13266503792208076}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19317794173579741, 'mae': 0.13298345997843997}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19272065749958664, 'mae': 0.1326576777838894}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1924052518972055, 'mae': 0.1323590654469332}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19238544094187593, 'mae': 0.13243116477234923}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.1933354799357229, 'mae': 0.13285012486114411}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19290963849433307, 'mae': 0.13247710242118738}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1923148485345309, 'mae': 0.13218068125284527}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19238531946987056, 'mae': 0.13231979902408628}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19351746379107237, 'mae': 0.13262786098929188}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19303410911809085, 'mae': 0.13238000935644814}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19232893653775923, 'mae': 0.13212158647341304}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1923889467219061, 'mae': 0.13226225059994667}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19445743391630094, 'mae': 0.13318581566234994}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19284617710233534, 'mae': 0.1324401985565474}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19254864982608721, 'mae': 0.13242083051414294}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1927098971741481, 'mae': 0.13249888664681483}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19487047897473295, 'mae': 0.13358058765051992}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1931247310748292, 'mae': 0.13250297763479515}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19278413006261885, 'mae': 0.1323839812950921}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19292482475542752, 'mae': 0.13248883256073807}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19571627290383956, 'mae': 0.13376659112659364}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19379319125826877, 'mae': 0.13257434945934765}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19303212110089218, 'mae': 0.1324646036251199}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19304978245897794, 'mae': 0.13249961006844338}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19615485629984442, 'mae': 0.13383654486570376}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19430979302984444, 'mae': 0.13290692203100626}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19325375235418785, 'mae': 0.13250372502587807}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1934221503193658, 'mae': 0.1326796699193383}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20223023359154926, 'mae': 0.13735004094134098}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19857521268006575, 'mae': 0.13494708613495346}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19500138525778005, 'mae': 0.13329472378674598}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19471266077457436, 'mae': 0.13341396651375215}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.20483065838361525, 'mae': 0.13894395528122822}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.20046900103659804, 'mae': 0.13653103288801088}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1966003899992586, 'mae': 0.13432054502571358}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19592640873579276, 'mae': 0.13398851662027025}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.20860069966364547, 'mae': 0.14105431564169557}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2030242579692987, 'mae': 0.13784785881645528}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19763490564097574, 'mae': 0.13510312387163334}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19672755561681057, 'mae': 0.1345187335173557}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.2110713051717518, 'mae': 0.14239140199286104}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.20486352993731358, 'mae': 0.13917010985513018}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19843396744731698, 'mae': 0.13582929288824477}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19780356592057247, 'mae': 0.13524897590436094}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.2297662151276473, 'mae': 0.14398437069322553}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.20960233654581215, 'mae': 0.14097806749621336}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20231682520391306, 'mae': 0.13770035489706917}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20174069914769774, 'mae': 0.13775800210894878}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.247898556262293, 'mae': 0.14875324095686002}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.21518315708603405, 'mae': 0.14492221619782647}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2050628194005812, 'mae': 0.13941351571570307}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.2045721984952687, 'mae': 0.1399053964084139}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.2657272748223959, 'mae': 0.1520455950019782}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.21967753679185736, 'mae': 0.14731106945962924}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.20771849401940576, 'mae': 0.1416113893381116}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.20747010070872285, 'mae': 0.14191148342499454}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.30600104118996196, 'mae': 0.15609734767717476}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.22468486087128842, 'mae': 0.15059769984227425}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.20985120215890485, 'mae': 0.14321776283080326}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.2098889426343844, 'mae': 0.14374443965534311}\n"
     ]
    }
   ],
   "source": [
    "best_rmse_5=1000\n",
    "best_mae_5=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_5) :\n",
    "                if(mae < best_mae_5) :\n",
    "                    best_rmse_5 = rmse\n",
    "                    best_mae_5 = mae\n",
    "                    best_learning_rate_5 = rate\n",
    "                    best_n_estimators_5 = estimator\n",
    "                    best_subsample_5 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19223114620400353"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13215340062000652"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mae_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_data['columns'] = X.columns\n",
    "experiment_data['result']={}\n",
    "experiment_data['result']['best_rmse']=best_rmse_5\n",
    "experiment_data['result']['best_mae'] = best_mae_5\n",
    "experiment_data['result']['learning_rate'] = best_learning_rate_5\n",
    "experiment_data['result']['n_estimators'] = best_n_estimators_5\n",
    "experiment_data['result']['subsample']=best_subsample_5\n",
    "experiment_data['date'] = str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_file = open('Experiment_Data.txt', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_file.write(\"\\n\"+str(experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'columns': Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
      "       'coleman_liau_index', 'automated_readability_index',\n",
      "       'dale_chall_readability_score', 'difficult_words',\n",
      "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
      "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
      "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
      "       'overall', 'help_pred_2'],\n",
      "      dtype='object'), 'result': {'best_rmse': 0.19223114620400353, 'best_mae': 0.13215340062000652, 'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9}, 'date': '2019-05-14 17:51:24.549776'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\"+str(experiment_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Experiment_Data.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'columns': Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\\n       'coleman_liau_index', 'automated_readability_index',\\n       'dale_chall_readability_score', 'difficult_words',\\n       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\\n       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\\n       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\\n       'overall', 'help_pred_1'],\\n      dtype='object'), 'result': {'best_rmse': 0.19219438320768384, 'best_mae': 0.13215792251807856, 'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9}, 'date': '2019-05-13 23:38:24.676568'}{'columns': Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\\n       'coleman_liau_index', 'automated_readability_index',\\n       'dale_chall_readability_score', 'difficult_words',\\n       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\\n       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\\n       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\\n       'overall', 'help_pred_1'],\\n      dtype='object'), 'result': {'best_rmse': 0.19219438320768384, 'best_mae': 0.13215792251807856, 'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9}, 'date': '2019-05-13 23:38:24.676568'}\""
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result = pd.read_csv('Featurewise_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>correlation</th>\n",
       "      <th>rmse-LR</th>\n",
       "      <th>mae-LR</th>\n",
       "      <th>rmse-RF</th>\n",
       "      <th>mae-RF</th>\n",
       "      <th>rmse-XGB</th>\n",
       "      <th>mae-XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>-0.111958</td>\n",
       "      <td>0.236515</td>\n",
       "      <td>0.172393</td>\n",
       "      <td>0.239447</td>\n",
       "      <td>0.173287</td>\n",
       "      <td>0.234108</td>\n",
       "      <td>0.170034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smog_index</td>\n",
       "      <td>0.115414</td>\n",
       "      <td>0.236336</td>\n",
       "      <td>0.172052</td>\n",
       "      <td>0.234847</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0.234702</td>\n",
       "      <td>0.170574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flesch_kincaid_grade</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.236547</td>\n",
       "      <td>0.172405</td>\n",
       "      <td>0.235316</td>\n",
       "      <td>0.171055</td>\n",
       "      <td>0.233818</td>\n",
       "      <td>0.169690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coleman_liau_index</td>\n",
       "      <td>0.107465</td>\n",
       "      <td>0.236695</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.236714</td>\n",
       "      <td>0.170643</td>\n",
       "      <td>0.233702</td>\n",
       "      <td>0.169747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>automated_readability_index</td>\n",
       "      <td>0.110973</td>\n",
       "      <td>0.236538</td>\n",
       "      <td>0.172404</td>\n",
       "      <td>0.238498</td>\n",
       "      <td>0.173384</td>\n",
       "      <td>0.233903</td>\n",
       "      <td>0.169729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dale_chall_readability_score</td>\n",
       "      <td>0.113564</td>\n",
       "      <td>0.236480</td>\n",
       "      <td>0.172404</td>\n",
       "      <td>0.238596</td>\n",
       "      <td>0.173142</td>\n",
       "      <td>0.233639</td>\n",
       "      <td>0.169768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>0.216660</td>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.170116</td>\n",
       "      <td>0.224072</td>\n",
       "      <td>0.162243</td>\n",
       "      <td>0.223898</td>\n",
       "      <td>0.161996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>linsear_write_formula</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.237968</td>\n",
       "      <td>0.173186</td>\n",
       "      <td>0.233313</td>\n",
       "      <td>0.168712</td>\n",
       "      <td>0.233643</td>\n",
       "      <td>0.169577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gunning_fog</td>\n",
       "      <td>0.109958</td>\n",
       "      <td>0.236567</td>\n",
       "      <td>0.172418</td>\n",
       "      <td>0.245639</td>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.233768</td>\n",
       "      <td>0.169728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>0.236107</td>\n",
       "      <td>0.172027</td>\n",
       "      <td>0.235254</td>\n",
       "      <td>0.171097</td>\n",
       "      <td>0.235211</td>\n",
       "      <td>0.171006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wps</td>\n",
       "      <td>0.109722</td>\n",
       "      <td>0.236569</td>\n",
       "      <td>0.172411</td>\n",
       "      <td>0.238453</td>\n",
       "      <td>0.173417</td>\n",
       "      <td>0.232971</td>\n",
       "      <td>0.169140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>review_length</td>\n",
       "      <td>0.196104</td>\n",
       "      <td>0.233316</td>\n",
       "      <td>0.170669</td>\n",
       "      <td>0.225790</td>\n",
       "      <td>0.163742</td>\n",
       "      <td>0.224070</td>\n",
       "      <td>0.162040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pos_no</td>\n",
       "      <td>0.223190</td>\n",
       "      <td>0.232012</td>\n",
       "      <td>0.169830</td>\n",
       "      <td>0.222123</td>\n",
       "      <td>0.159747</td>\n",
       "      <td>0.222092</td>\n",
       "      <td>0.159673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>neg_no</td>\n",
       "      <td>0.119384</td>\n",
       "      <td>0.236172</td>\n",
       "      <td>0.172354</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.171108</td>\n",
       "      <td>0.234667</td>\n",
       "      <td>0.171065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>user_deviation</td>\n",
       "      <td>-0.330468</td>\n",
       "      <td>0.224785</td>\n",
       "      <td>0.160081</td>\n",
       "      <td>0.251740</td>\n",
       "      <td>0.171661</td>\n",
       "      <td>0.224415</td>\n",
       "      <td>0.159584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>user_delay</td>\n",
       "      <td>-0.094694</td>\n",
       "      <td>0.236995</td>\n",
       "      <td>0.172626</td>\n",
       "      <td>0.239586</td>\n",
       "      <td>0.170837</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.172250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>no_of_reviews</td>\n",
       "      <td>0.071760</td>\n",
       "      <td>0.237384</td>\n",
       "      <td>0.173439</td>\n",
       "      <td>0.236534</td>\n",
       "      <td>0.173950</td>\n",
       "      <td>0.236547</td>\n",
       "      <td>0.173906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>reviewer_days</td>\n",
       "      <td>0.091997</td>\n",
       "      <td>0.236926</td>\n",
       "      <td>0.172384</td>\n",
       "      <td>0.238912</td>\n",
       "      <td>0.172622</td>\n",
       "      <td>0.236160</td>\n",
       "      <td>0.171822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stem_sim_length</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.234537</td>\n",
       "      <td>0.170971</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.231948</td>\n",
       "      <td>0.168446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lem_sim_length</td>\n",
       "      <td>0.171916</td>\n",
       "      <td>0.234496</td>\n",
       "      <td>0.170931</td>\n",
       "      <td>0.231958</td>\n",
       "      <td>0.168378</td>\n",
       "      <td>0.231941</td>\n",
       "      <td>0.168347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>overall</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.210184</td>\n",
       "      <td>0.146280</td>\n",
       "      <td>0.208644</td>\n",
       "      <td>0.144121</td>\n",
       "      <td>0.208644</td>\n",
       "      <td>0.144111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature  correlation   rmse-LR    mae-LR   rmse-RF  \\\n",
       "0            flesch_reading_ease    -0.111958  0.236515  0.172393  0.239447   \n",
       "1                     smog_index     0.115414  0.236336  0.172052  0.234847   \n",
       "2           flesch_kincaid_grade     0.110595  0.236547  0.172405  0.235316   \n",
       "3             coleman_liau_index     0.107465  0.236695  0.172458  0.236714   \n",
       "4    automated_readability_index     0.110973  0.236538  0.172404  0.238498   \n",
       "5   dale_chall_readability_score     0.113564  0.236480  0.172404  0.238596   \n",
       "6                difficult_words     0.216660  0.232292  0.170116  0.224072   \n",
       "7          linsear_write_formula     0.014142  0.237968  0.173186  0.233313   \n",
       "8                    gunning_fog     0.109958  0.236567  0.172418  0.245639   \n",
       "9                 sentence_count     0.121914  0.236107  0.172027  0.235254   \n",
       "10                           wps     0.109722  0.236569  0.172411  0.238453   \n",
       "11                 review_length     0.196104  0.233316  0.170669  0.225790   \n",
       "12                        pos_no     0.223190  0.232012  0.169830  0.222123   \n",
       "13                        neg_no     0.119384  0.236172  0.172354  0.234694   \n",
       "14                user_deviation    -0.330468  0.224785  0.160081  0.251740   \n",
       "15                    user_delay    -0.094694  0.236995  0.172626  0.239586   \n",
       "16                 no_of_reviews     0.071760  0.237384  0.173439  0.236534   \n",
       "17                 reviewer_days     0.091997  0.236926  0.172384  0.238912   \n",
       "18               stem_sim_length     0.170400  0.234537  0.170971  0.231976   \n",
       "19                lem_sim_length     0.171916  0.234496  0.170931  0.231958   \n",
       "20                       overall     0.475150  0.210184  0.146280  0.208644   \n",
       "\n",
       "      mae-RF  rmse-XGB   mae-XGB  \n",
       "0   0.173287  0.234108  0.170034  \n",
       "1   0.170670  0.234702  0.170574  \n",
       "2   0.171055  0.233818  0.169690  \n",
       "3   0.170643  0.233702  0.169747  \n",
       "4   0.173384  0.233903  0.169729  \n",
       "5   0.173142  0.233639  0.169768  \n",
       "6   0.162243  0.223898  0.161996  \n",
       "7   0.168712  0.233643  0.169577  \n",
       "8   0.177515  0.233768  0.169728  \n",
       "9   0.171097  0.235211  0.171006  \n",
       "10  0.173417  0.232971  0.169140  \n",
       "11  0.163742  0.224070  0.162040  \n",
       "12  0.159747  0.222092  0.159673  \n",
       "13  0.171108  0.234667  0.171065  \n",
       "14  0.171661  0.224415  0.159584  \n",
       "15  0.170837  0.236607  0.172250  \n",
       "16  0.173950  0.236547  0.173906  \n",
       "17  0.172622  0.236160  0.171822  \n",
       "18  0.168500  0.231948  0.168446  \n",
       "19  0.168378  0.231941  0.168347  \n",
       "20  0.144121  0.208644  0.144111  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurewise_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_least_error(lr, rf, xgb) :\n",
    "    if xgb < rf and xgb < lr :\n",
    "        return xgb\n",
    "    if rf < lr and rf < xgb :\n",
    "        return rf\n",
    "    if lr < rf and lr < xgb :\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result['least_mae'] = featurewise_result.apply(lambda row: get_least_error(row['mae-LR'],row['mae-RF'], row['mae-XGB']), axis=1)\n",
    "featurewise_result['least_rmse'] = featurewise_result.apply(lambda row: get_least_error(row['rmse-LR'], row['rmse-RF'], row['rmse-XGB']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result_sorted = featurewise_result.sort_values(by='least_mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featurewise_result_sorted.to_csv('Featurewise_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
      "       'coleman_liau_index', 'automated_readability_index',\n",
      "       'dale_chall_readability_score', 'difficult_words',\n",
      "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
      "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
      "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
      "       'overall'],\n",
      "      dtype='object')\n",
      "No of Features:  21\n"
     ]
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "print(\"Features: \",X.columns)\n",
    "print(\"No of Features: \",len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count   ...     review_length  \\\n",
       "0              14.000000       176.92               1   ...               427   \n",
       "1               8.666667       175.64               2   ...               846   \n",
       "2               7.666667        13.01              23   ...               449   \n",
       "3               7.200000        14.00               5   ...                64   \n",
       "4               4.400000        61.55               1   ...               138   \n",
       "\n",
       "   pos_no  neg_no  user_deviation  user_delay  no_of_reviews  reviewer_days  \\\n",
       "0       2       4        1.399189  10152000.0              1          260.0   \n",
       "1      18       8        0.624437  10886400.0              1         1390.0   \n",
       "2       7       6        0.510375   7516800.0              1          929.0   \n",
       "3       2       0        0.442171   2678400.0              1            0.0   \n",
       "4       1       3        1.616074   7603200.0              1            0.0   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(learning_rate=grid_learning_rate, n_estimators=grid_n_estimators, subsample=grid_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.0368974164299238\n",
      "Root Mean Squared Error (RMSE): 0.1920870022409736\n",
      "Mean Absolute Error (MAE):      0.13192014023385665\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_list = abs(y_test-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df['y_test']=y_test\n",
    "error_df['predictions'] = predictions\n",
    "error_df['error'] = error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>predictions</th>\n",
       "      <th>error</th>\n",
       "      <th>class_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69520</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926131</td>\n",
       "      <td>0.926131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918928</td>\n",
       "      <td>0.918928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74862</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915146</td>\n",
       "      <td>0.915146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61109</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912949</td>\n",
       "      <td>0.912949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42479</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911104</td>\n",
       "      <td>0.911104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30207</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909286</td>\n",
       "      <td>0.909286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62249</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.931933</td>\n",
       "      <td>0.904906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26938</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904504</td>\n",
       "      <td>0.904504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61105</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901851</td>\n",
       "      <td>0.901851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22696</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896530</td>\n",
       "      <td>0.896530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71653</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57389</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892782</td>\n",
       "      <td>0.892782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30212</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888650</td>\n",
       "      <td>0.888650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36783</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884479</td>\n",
       "      <td>0.884479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15368</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884155</td>\n",
       "      <td>0.884155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881216</td>\n",
       "      <td>0.881216</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37163</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880677</td>\n",
       "      <td>0.880677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75408</th>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.894958</td>\n",
       "      <td>0.878829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124452</td>\n",
       "      <td>0.875548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64005</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870496</td>\n",
       "      <td>0.870496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81257</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867911</td>\n",
       "      <td>0.867911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8320</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867749</td>\n",
       "      <td>0.867749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25109</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>0.865477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56194</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.929098</td>\n",
       "      <td>0.862431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9632</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.859079</td>\n",
       "      <td>0.859079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17033</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858829</td>\n",
       "      <td>0.858829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63966</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857232</td>\n",
       "      <td>0.857232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48132</th>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.926851</td>\n",
       "      <td>0.854909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74104</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843250</td>\n",
       "      <td>0.843250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64758</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.904258</td>\n",
       "      <td>0.841758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.841478</td>\n",
       "      <td>0.841478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58170</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.893866</td>\n",
       "      <td>0.841234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57243</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.940104</td>\n",
       "      <td>0.840104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73581</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.865344</td>\n",
       "      <td>0.838317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38809</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833748</td>\n",
       "      <td>0.833748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66461</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.903539</td>\n",
       "      <td>0.832111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56498</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.932028</td>\n",
       "      <td>0.832028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27032</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828635</td>\n",
       "      <td>0.828635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44684</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.894948</td>\n",
       "      <td>0.828281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20675</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828264</td>\n",
       "      <td>0.828264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_test  predictions     error  class_predict\n",
       "69520  0.000000     0.926131  0.926131              1\n",
       "5371   0.000000     0.918928  0.918928              1\n",
       "74862  0.000000     0.915146  0.915146              1\n",
       "61109  0.000000     0.912949  0.912949              1\n",
       "42479  0.000000     0.911104  0.911104              1\n",
       "30207  0.000000     0.909286  0.909286              1\n",
       "62249  0.027027     0.931933  0.904906              1\n",
       "26938  0.000000     0.904504  0.904504              1\n",
       "61105  0.000000     0.901851  0.901851              1\n",
       "22696  0.000000     0.896530  0.896530              1\n",
       "71653  0.000000     0.893957  0.893957              1\n",
       "57389  0.000000     0.892782  0.892782              1\n",
       "30212  0.000000     0.888650  0.888650              1\n",
       "36783  0.000000     0.884479  0.884479              1\n",
       "15368  0.000000     0.884155  0.884155              1\n",
       "30208  0.000000     0.881216  0.881216              1\n",
       "37163  0.000000     0.880677  0.880677              1\n",
       "75408  0.016129     0.894958  0.878829              0\n",
       "14416  1.000000     0.124452  0.875548              0\n",
       "64005  0.000000     0.870496  0.870496              1\n",
       "81257  0.000000     0.867911  0.867911              1\n",
       "8320   0.000000     0.867749  0.867749              1\n",
       "25109  0.066667     0.932143  0.865477              1\n",
       "56194  0.066667     0.929098  0.862431              1\n",
       "9632   0.000000     0.859079  0.859079              1\n",
       "17033  0.000000     0.858829  0.858829              1\n",
       "63966  0.000000     0.857232  0.857232              1\n",
       "48132  0.071942     0.926851  0.854909              1\n",
       "74104  0.000000     0.843250  0.843250              1\n",
       "64758  0.062500     0.904258  0.841758              0\n",
       "63745  0.000000     0.841478  0.841478              1\n",
       "58170  0.052632     0.893866  0.841234              1\n",
       "57243  0.100000     0.940104  0.840104              1\n",
       "73581  0.027027     0.865344  0.838317              1\n",
       "38809  0.000000     0.833748  0.833748              0\n",
       "66461  0.071429     0.903539  0.832111              1\n",
       "56498  0.100000     0.932028  0.832028              1\n",
       "27032  0.000000     0.828635  0.828635              1\n",
       "44684  0.066667     0.894948  0.828281              1\n",
       "20675  0.000000     0.828264  0.828264              1"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_test           2434\n",
       "predictions      2434\n",
       "error            2434\n",
       "class_predict    2434\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[(error_df['class_predict']==0) & (error_df['predictions'] > 0.5) & (error_df['y_test'] <= 0.5)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_test           3171\n",
       "predictions      3171\n",
       "error            3171\n",
       "class_predict    3171\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[(error_df['predictions']>0.5) & (error_df['y_test']<=0.5)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df['class_predict']=helper_df['help_pred_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08892003573785723"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[error_df['class_predict']==1]['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df = error_df.sort_values(by=['error'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18934951076361148"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[error_df['error']> 0.5]['y_test'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1319181610444203"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[(error_df['y_test']>=0.0) & (error_df['predictions']<=1.0) & (error_df['predictions']>=0.0)]['error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3882"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_df[error_df['y_test']<=0.5]['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_test           2371\n",
       "predictions      2371\n",
       "error            2371\n",
       "class_predict    2371\n",
       "dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[(error_df['y_test']>0.9) & (error_df['class_predict']==0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10134"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[error_df['class_predict']==0]['error'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32510"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.940941248846508"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3882*100/32510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12258064516129032"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df[error_df['y_test']==0.0]['class_predict'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_revised_score(pred, label) :\n",
    "    return pred + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_revised_score(pred, label) :\n",
    "    pred = pred+0.05\n",
    "    if pred > 1 :\n",
    "        pred =1\n",
    "    if pred < 0 :\n",
    "        pred = 0\n",
    "    if label == 0 and pred >= 0.9:\n",
    "        pred = pred - 0.02\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-529-15a77bfd0ad5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-529-15a77bfd0ad5>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    if pred <0 :\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "  if pred > 1 :\n",
    "        pred =1\n",
    "    if pred <0 :\n",
    "        pred =0\n",
    "    if label==1 :\n",
    "        return pred\n",
    "    if pred <0.5 :\n",
    "        return pred\n",
    "    if pred >= 0.9 :\n",
    "        return pred -0.015\n",
    "    return pred + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_revised_score(0.9,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df['revised_score'] = error_df.apply(lambda row : get_revised_score(row['predictions'], row['class_predict']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_df['error_revised'] = abs(error_df['revised_score']-error_df['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12806866003726933"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df['error_revised'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Inter-Feature correlation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['smog_index', 'coleman_liau_index', 'difficult_words',\n",
      "       'linsear_write_formula', 'sentence_count', 'wps', 'pos_no',\n",
      "       'user_deviation', 'user_delay', 'no_of_reviews', 'reviewer_days',\n",
      "       'lem_sim_length', 'overall'],\n",
      "      dtype='object')\n",
      "No of Features:  13\n"
     ]
    }
   ],
   "source": [
    "X = text_df[['smog_index', 'coleman_liau_index',\n",
    "       'difficult_words', 'linsear_write_formula',\n",
    "       'sentence_count', 'wps', 'pos_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['lem_sim_length','overall']])\n",
    "\n",
    "print(\"Features: \",X.columns)\n",
    "print(\"No of Features: \",len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smog_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>wps</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.69</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.72</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.3</td>\n",
       "      <td>5.92</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>23</td>\n",
       "      <td>19.521739</td>\n",
       "      <td>7</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.8</td>\n",
       "      <td>8.05</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.09</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   smog_index  coleman_liau_index  difficult_words  linsear_write_formula  \\\n",
       "0         0.0                7.69               44              14.000000   \n",
       "1         0.0                9.72               94               8.666667   \n",
       "2         7.3                5.92               36               7.666667   \n",
       "3        10.8                8.05               11               7.200000   \n",
       "4         0.0                8.09               15               4.400000   \n",
       "\n",
       "   sentence_count         wps  pos_no  user_deviation  user_delay  \\\n",
       "0               1  427.000000       2        1.399189  10152000.0   \n",
       "1               2  423.000000      18        0.624437  10886400.0   \n",
       "2              23   19.521739       7        0.510375   7516800.0   \n",
       "3               5   12.800000       2        0.442171   2678400.0   \n",
       "4               1  138.000000       1        1.616074   7603200.0   \n",
       "\n",
       "   no_of_reviews  reviewer_days  lem_sim_length  overall  \n",
       "0              1          260.0              13      1.0  \n",
       "1              1         1390.0              23      3.0  \n",
       "2              1          929.0              16      2.0  \n",
       "3              1            0.0               2      5.0  \n",
       "4              1            0.0               2      1.0  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03740666324834456\n",
      "Root Mean Squared Error (RMSE): 0.19340802270936064\n",
      "Mean Absolute Error (MAE):      0.13326267738249728\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20171749953146403, 'mae': 0.1559526248510373}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.2019182641557227, 'mae': 0.15609815593138562}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.2021600676265661, 'mae': 0.15625301169167322}\n",
      "{'learning_rate': 0.01, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.20221236117143457, 'mae': 0.1563045985390589}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19590809982366242, 'mae': 0.14118442189511274}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.196056843817347, 'mae': 0.14132035025056605}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19630457507962856, 'mae': 0.14153463879714134}\n",
      "{'learning_rate': 0.01, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.1963364140473708, 'mae': 0.14158779812269956}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19451788263895053, 'mae': 0.13636049424516578}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19461320007261898, 'mae': 0.13650232033285714}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19483073439534004, 'mae': 0.13674779116710944}\n",
      "{'learning_rate': 0.01, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19491848146258015, 'mae': 0.13685695365654466}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19397621529231746, 'mae': 0.13452351411409855}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19401857011250193, 'mae': 0.1346774752566921}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19419649575704326, 'mae': 0.13494815355671394}\n",
      "{'learning_rate': 0.01, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1942947569897271, 'mae': 0.1350889571920297}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.193417853382685, 'mae': 0.13283805572799404}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19303377520444676, 'mae': 0.1327407662032092}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19268692846740298, 'mae': 0.1325788312121777}\n",
      "{'learning_rate': 0.09, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19289723203997824, 'mae': 0.13280147170939632}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19318501371923297, 'mae': 0.13288344872416616}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19289091397640487, 'mae': 0.13273243462791384}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.19252411961707316, 'mae': 0.13239042957196814}\n",
      "{'learning_rate': 0.09, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19247633421314375, 'mae': 0.13235415665939965}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19326390125566162, 'mae': 0.13270432770637317}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19289775869680442, 'mae': 0.13247892276695958}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19238426132140946, 'mae': 0.1321799530943447}\n",
      "{'learning_rate': 0.09, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19239307448561865, 'mae': 0.13222640432424163}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19336803333844121, 'mae': 0.13260963783257554}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19294090187780324, 'mae': 0.13235530947263227}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19235112674585303, 'mae': 0.1321202629181903}\n",
      "{'learning_rate': 0.09, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19235466663318918, 'mae': 0.13215525734099878}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.19312564220734768, 'mae': 0.13262263274377867}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19277364908102937, 'mae': 0.13252418457244267}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19279364750416247, 'mae': 0.13260250202055163}\n",
      "{'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.19291776410718967, 'mae': 0.1327473219661155}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19316165864922943, 'mae': 0.13274885692755087}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1927002152015459, 'mae': 0.13252928143065035}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1925455436627403, 'mae': 0.13232256594152797}\n",
      "{'learning_rate': 0.1, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19266312968313656, 'mae': 0.13244523017489782}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19316050651923922, 'mae': 0.1325793089982664}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.1928362830299161, 'mae': 0.1323176207181173}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1924636325932108, 'mae': 0.13215394324508026}\n",
      "{'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.19266449740656674, 'mae': 0.13237902540254956}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.1933838017492289, 'mae': 0.13249863885886096}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19285488594443514, 'mae': 0.13220101607477963}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19244702242981604, 'mae': 0.13213110410074164}\n",
      "{'learning_rate': 0.1, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1926734112185432, 'mae': 0.13231267286651607}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.194309081509221, 'mae': 0.13299849079999435}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.1931817886619002, 'mae': 0.13245797594123357}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19252742630959924, 'mae': 0.13221998824419887}\n",
      "{'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1927179403918059, 'mae': 0.13235857989942146}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.19478338016097618, 'mae': 0.1335204381607028}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.1933177584864187, 'mae': 0.132804066319753}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1928182122873475, 'mae': 0.13226037975951757}\n",
      "{'learning_rate': 0.2, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19282750995590905, 'mae': 0.13231658478644398}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.19538957358420633, 'mae': 0.13365245479377225}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.19396257456055022, 'mae': 0.1329335729831072}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.19292975902962345, 'mae': 0.13218562674456852}\n",
      "{'learning_rate': 0.2, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.193065672954025, 'mae': 0.13236555941669448}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.19600782296773409, 'mae': 0.13374794234686396}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.19420589009163486, 'mae': 0.13304029657591537}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.1930311545709206, 'mae': 0.13219486831811433}\n",
      "{'learning_rate': 0.2, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.19316716684891466, 'mae': 0.1323863371624234}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.20127305743901922, 'mae': 0.13688114361529738}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.19803402895731137, 'mae': 0.13514295439427718}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.19527680451672833, 'mae': 0.13356133442036566}\n",
      "{'learning_rate': 0.5, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1947496385585862, 'mae': 0.13346806844935805}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.20475372724758564, 'mae': 0.13930447819246555}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.19957570446701223, 'mae': 0.13651999346591}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.1965100356582403, 'mae': 0.13440946616940608}\n",
      "{'learning_rate': 0.5, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.19588264878982178, 'mae': 0.1341691116738836}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.20748596324331917, 'mae': 0.1404088137676256}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2014748355094838, 'mae': 0.1376974424967732}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.1976755570488564, 'mae': 0.13524442418613897}\n",
      "{'learning_rate': 0.5, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.1966721360288147, 'mae': 0.13469655884682644}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.20948320232368595, 'mae': 0.14162660145293868}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.20267749254337586, 'mae': 0.13846300391836042}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.19846149566971472, 'mae': 0.13573903929570139}\n",
      "{'learning_rate': 0.5, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.1974792108568879, 'mae': 0.1352920657160884}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.3, 'rmse': 0.21868805765811714, 'mae': 0.14408527309472027}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.5, 'rmse': 0.20779665230515496, 'mae': 0.14005104757440462}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 0.9, 'rmse': 0.20065351089809266, 'mae': 0.13691654574244902}\n",
      "{'learning_rate': 0.9, 'n_estimators': 200, 'subsample': 1, 'rmse': 0.1999802618557771, 'mae': 0.13702700610687232}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.3, 'rmse': 0.22681944222091321, 'mae': 0.14877982044995064}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.5, 'rmse': 0.2127144907081161, 'mae': 0.14402118633076585}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 0.9, 'rmse': 0.2038343988456319, 'mae': 0.1390945283763627}\n",
      "{'learning_rate': 0.9, 'n_estimators': 300, 'subsample': 1, 'rmse': 0.20253209411193981, 'mae': 0.13899812661122068}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.3, 'rmse': 0.23689276039294102, 'mae': 0.15097466126485984}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.5, 'rmse': 0.2171919222029803, 'mae': 0.14641149482945848}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 0.9, 'rmse': 0.2059652431254962, 'mae': 0.14060296776554707}\n",
      "{'learning_rate': 0.9, 'n_estimators': 400, 'subsample': 1, 'rmse': 0.2047714129845543, 'mae': 0.1404079228956475}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.3, 'rmse': 0.2442899312297964, 'mae': 0.15515058058279804}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.5, 'rmse': 0.2216327569093658, 'mae': 0.14950429648598107}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 0.9, 'rmse': 0.20822984132445524, 'mae': 0.14249884016863829}\n",
      "{'learning_rate': 0.9, 'n_estimators': 500, 'subsample': 1, 'rmse': 0.2064956303114996, 'mae': 0.14188902291834388}\n"
     ]
    }
   ],
   "source": [
    "best_rmse_6=1000\n",
    "best_mae_6=1000\n",
    "for rate in params['learning_rate'] :\n",
    "    for estimator in params['n_estimators'] :\n",
    "        for subsample in params['subsample'] :\n",
    "            xgb_model = xgb.XGBRegressor(learning_rate= rate, n_estimators = estimator, subsample=subsample)\n",
    "            xgb_model.fit(X_train,y_train)\n",
    "            predictions=xgb_model.predict(X_test)\n",
    "            rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "            mae=mean_absolute_error(y_test,predictions)\n",
    "            \n",
    "            res_dict = {'learning_rate': rate, 'n_estimators': estimator, 'subsample': subsample, 'rmse' : rmse, 'mae' : mae }\n",
    "            result_df.append(res_dict, ignore_index=True)\n",
    "\n",
    "            print(res_dict)\n",
    "            \n",
    "            if(rmse<=best_rmse_6) :\n",
    "                if(mae < best_mae_6) :\n",
    "                    best_rmse_6 = rmse\n",
    "                    best_mae_6 = mae\n",
    "                    best_learning_rate_6 = rate\n",
    "                    best_n_estimators_6 = estimator\n",
    "                    best_subsample_6 = subsample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1321202629181903"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mae_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment : Mean Difference of Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  Index(['flesch_reading_ease', 'smog_index', 'flesch_kincaid_grade',\n",
      "       'coleman_liau_index', 'automated_readability_index',\n",
      "       'dale_chall_readability_score', 'difficult_words',\n",
      "       'linsear_write_formula', 'gunning_fog', 'sentence_count', 'wps',\n",
      "       'review_length', 'pos_no', 'neg_no', 'user_deviation', 'user_delay',\n",
      "       'no_of_reviews', 'reviewer_days', 'stem_sim_length', 'lem_sim_length',\n",
      "       'overall'],\n",
      "      dtype='object')\n",
      "No of Features:  21\n"
     ]
    }
   ],
   "source": [
    "X = text_df[['flesch_reading_ease', \n",
    "       'smog_index', 'flesch_kincaid_grade', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'dale_chall_readability_score',\n",
    "       'difficult_words', 'linsear_write_formula', 'gunning_fog',\n",
    "       'sentence_count', 'wps', 'review_length', 'pos_no', 'neg_no']]\n",
    "\n",
    "X=X.join(user_df[['user_deviation', 'user_delay','no_of_reviews', 'reviewer_days']])\n",
    "\n",
    "X = X.join(meta_df[['stem_sim_length','lem_sim_length','overall']])\n",
    "\n",
    "print(\"Features: \",X.columns)\n",
    "print(\"No of Features: \",len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>...</th>\n",
       "      <th>review_length</th>\n",
       "      <th>pos_no</th>\n",
       "      <th>neg_no</th>\n",
       "      <th>user_deviation</th>\n",
       "      <th>user_delay</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>reviewer_days</th>\n",
       "      <th>stem_sim_length</th>\n",
       "      <th>lem_sim_length</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-336.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.3</td>\n",
       "      <td>7.69</td>\n",
       "      <td>211.1</td>\n",
       "      <td>26.44</td>\n",
       "      <td>44</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>176.92</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>427</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.399189</td>\n",
       "      <td>10152000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>260.0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-340.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.9</td>\n",
       "      <td>9.72</td>\n",
       "      <td>210.8</td>\n",
       "      <td>26.37</td>\n",
       "      <td>94</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>175.64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>846</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>10886400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.52</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.92</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.87</td>\n",
       "      <td>36</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>13.01</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>449</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>7516800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>929.0</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.94</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>11</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>14.00</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442171</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6</td>\n",
       "      <td>8.09</td>\n",
       "      <td>67.2</td>\n",
       "      <td>12.20</td>\n",
       "      <td>15</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>61.55</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.616074</td>\n",
       "      <td>7603200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  smog_index  flesch_kincaid_grade  coleman_liau_index  \\\n",
       "0              -336.56         0.0                 166.3                7.69   \n",
       "1              -340.96         0.0                 165.9                9.72   \n",
       "2                85.52         7.3                   6.2                5.92   \n",
       "3                66.94        10.8                   7.1                8.05   \n",
       "4               -43.22         0.0                  53.6                8.09   \n",
       "\n",
       "   automated_readability_index  dale_chall_readability_score  difficult_words  \\\n",
       "0                        211.1                         26.44               44   \n",
       "1                        210.8                         26.37               94   \n",
       "2                          7.2                          5.87               36   \n",
       "3                          6.3                          6.99               11   \n",
       "4                         67.2                         12.20               15   \n",
       "\n",
       "   linsear_write_formula  gunning_fog  sentence_count   ...     review_length  \\\n",
       "0              14.000000       176.92               1   ...               427   \n",
       "1               8.666667       175.64               2   ...               846   \n",
       "2               7.666667        13.01              23   ...               449   \n",
       "3               7.200000        14.00               5   ...                64   \n",
       "4               4.400000        61.55               1   ...               138   \n",
       "\n",
       "   pos_no  neg_no  user_deviation  user_delay  no_of_reviews  reviewer_days  \\\n",
       "0       2       4        1.399189  10152000.0              1          260.0   \n",
       "1      18       8        0.624437  10886400.0              1         1390.0   \n",
       "2       7       6        0.510375   7516800.0              1          929.0   \n",
       "3       2       0        0.442171   2678400.0              1            0.0   \n",
       "4       1       3        1.616074   7603200.0              1            0.0   \n",
       "\n",
       "   stem_sim_length  lem_sim_length  overall  \n",
       "0               15              13      1.0  \n",
       "1               24              23      3.0  \n",
       "2               17              16      2.0  \n",
       "3                2               2      5.0  \n",
       "4                2               2      1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = review_df['helpfulness_score']\n",
    "\t\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(learning_rate=grid_learning_rate, n_estimators=grid_n_estimators, subsample=grid_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-538-4e2f5b53247b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_learning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_n_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_subsample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Best Learning Rate : {} , Best No of Estimators : {} , Best Subsample size : {}\".format(best_learning_rate, best_n_estimators, best_subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.09, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.0368974164299238\n",
      "Root Mean Squared Error (RMSE): 0.1920870022409736\n",
      "Mean Absolute Error (MAE):      0.13192014023385665\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,predictions)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,predictions))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,predictions)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train = xgb_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_train = preds_train-y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.7907002463167557e-05\n"
     ]
    }
   ],
   "source": [
    "print(err_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train_2 = preds_train - err_train.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_train_2 =preds_train_2 - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036126155277778405"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_train.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.849799324826762e-08"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_train_2.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14019    0.072296\n",
       "2513    -0.107392\n",
       "67575   -0.009882\n",
       "27635    0.202600\n",
       "35219    0.019037\n",
       "19071   -0.098061\n",
       "74043   -0.009664\n",
       "32637    0.019351\n",
       "40561    0.114297\n",
       "57269    0.493940\n",
       "8604    -0.217952\n",
       "61705   -0.127001\n",
       "19452   -0.032631\n",
       "31444   -0.008109\n",
       "24728    0.088135\n",
       "32691    0.178584\n",
       "71632    0.300440\n",
       "5637     0.193927\n",
       "50247   -0.289423\n",
       "56123    0.414839\n",
       "38520   -0.096079\n",
       "29493   -0.136129\n",
       "33247   -0.044369\n",
       "73743    0.108189\n",
       "27047   -0.092839\n",
       "73504    0.295462\n",
       "77545    0.042740\n",
       "31348   -0.193481\n",
       "30865   -0.068068\n",
       "56413   -0.100964\n",
       "           ...   \n",
       "21311   -0.101127\n",
       "21459   -0.084898\n",
       "3748     0.514119\n",
       "71919   -0.282299\n",
       "77257   -0.051957\n",
       "74071    0.385073\n",
       "63083   -0.029655\n",
       "20191   -0.047516\n",
       "15436    0.070426\n",
       "28488   -0.268457\n",
       "9772    -0.079731\n",
       "18491    0.314923\n",
       "53619   -0.082498\n",
       "78447   -0.274420\n",
       "63315    0.341613\n",
       "32817   -0.123498\n",
       "19566   -0.030091\n",
       "78556    0.508471\n",
       "71996   -0.156775\n",
       "61173   -0.024932\n",
       "35391   -0.053373\n",
       "22056   -0.022553\n",
       "17357   -0.146627\n",
       "20463   -0.002380\n",
       "77899    0.460892\n",
       "55293   -0.079426\n",
       "49751   -0.068437\n",
       "5695    -0.079956\n",
       "73542    0.023727\n",
       "45919    0.034306\n",
       "Name: helpfulness_score, Length: 48765, dtype: float64"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train_3=pd.Series(preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8818578124046326"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_train_3[preds_train_3-y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_2 = predictions - err_train.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.038341095974403896\n",
      "Root Mean Squared Error (RMSE): 0.19580882506772745\n",
      "Mean Absolute Error (MAE):      0.12734134337808103\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,preds_2)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,preds_2))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,preds_2)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_err_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_err_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_err_df['preds_train'] = preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff = preds_train - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_err_df['y_train'] = y_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_list=y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_err_df['err_train'] = diff.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.036126155277778405"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_err_df['err_train'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preds_train    0.883305\n",
       "err_train     -0.030175\n",
       "y_train        0.913480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_err_df[(new_err_df['err_train']<=-0.02) & (new_err_df['err_train'] >= -0.04) ].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preds_train    48765\n",
       "err_train      48765\n",
       "y_train        48765\n",
       "dtype: int64"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_err_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def revise_preds(pred) :\n",
    "    if pred >=0.880 and pred <=0.889 :\n",
    "        return pred - err_train.median()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####new_err_df['revised_preds'] = new_err_df.apply(lambda row: get_least_error(row['mae-LR'],row['mae-RF'], row['mae-XGB']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_ser = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_3 = predictions_ser.apply(revise_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.939419\n",
       "1    0.915579\n",
       "2    0.932346\n",
       "3    0.871317\n",
       "4    0.870685\n",
       "dtype: float64"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):       0.03694080933291586\n",
      "Root Mean Squared Error (RMSE): 0.1921999202208884\n",
      "Mean Absolute Error (MAE):      0.13159618640264975\n"
     ]
    }
   ],
   "source": [
    "# MSE : Mean Squared Error\n",
    "mse=mean_squared_error(y_test,preds_3)\n",
    "\n",
    "# RMSE : Root Mean Squared Error\n",
    "rmse=math.sqrt(mean_squared_error(y_test,preds_3))\n",
    "\n",
    "# MAE : Mean Absolute Error\n",
    "mae=mean_absolute_error(y_test,preds_3)\n",
    "\n",
    "print('Mean Squared Error (MSE):      ',mse)\n",
    "print('Root Mean Squared Error (RMSE):',rmse)\n",
    "print('Mean Absolute Error (MAE):     ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    callback\n",
      "    compat\n",
      "    core\n",
      "    libpath\n",
      "    plotting\n",
      "    rabit\n",
      "    sklearn\n",
      "    training\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        xgboost.core.Booster\n",
      "        xgboost.core.DMatrix\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        xgboost.sklearn.XGBModel\n",
      "            xgboost.sklearn.XGBClassifier(xgboost.sklearn.XGBModel, sklearn.base.ClassifierMixin)\n",
      "            xgboost.sklearn.XGBRanker\n",
      "            xgboost.sklearn.XGBRegressor(xgboost.sklearn.XGBModel, sklearn.base.RegressorMixin)\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  A Booster of XGBoost.\n",
      "     |  \n",
      "     |  Booster is the model of xgboost, that contains low level routines for\n",
      "     |  training, prediction and evaluation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, cache=(), model_file=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          Parameters for boosters.\n",
      "     |      cache : list\n",
      "     |          List of cache items.\n",
      "     |      model_file : string\n",
      "     |          Path to the model file.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key to get attribute from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : str\n",
      "     |          The attribute value of the key, returns None if attribute do not exist.\n",
      "     |  \n",
      "     |  attributes(self)\n",
      "     |      Get attributes stored in the Booster as a dictionary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      "     |          Returns an empty dict if there's no attributes.\n",
      "     |  \n",
      "     |  boost(self, dtrain, grad, hess)\n",
      "     |      Boost the booster for one iteration, with customized gradient statistics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          The training DMatrix.\n",
      "     |      grad : list\n",
      "     |          The first order of gradient.\n",
      "     |      hess : list\n",
      "     |          The second order of gradient.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Copy the booster object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster: `Booster`\n",
      "     |          a copied booster model\n",
      "     |  \n",
      "     |  dump_model(self, fout, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Dump model into a text or JSON file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fout : string\n",
      "     |          Output file name.\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump file. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  eval(self, data, name='eval', iteration=0)\n",
      "     |      Evaluate the model on mat.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      name : str, optional\n",
      "     |          The name of the dataset.\n",
      "     |      \n",
      "     |      iteration : int, optional\n",
      "     |          The current iteration number.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  eval_set(self, evals, iteration=0, feval=None)\n",
      "     |      Evaluate a set of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      evals : list of tuples (DMatrix, string)\n",
      "     |          List of items to be evaluated.\n",
      "     |      iteration : int\n",
      "     |          Current iteration.\n",
      "     |      feval : function\n",
      "     |          Custom evaluation function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Returns the model dump as a list of strings.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  get_fscore(self, fmap='')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      .. note:: Zero-importance features will not be included\n",
      "     |      \n",
      "     |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      "     |         those features that have not been used in any split conditions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_score(self, fmap='', importance_type='weight')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      Importance type can be defined as:\n",
      "     |      \n",
      "     |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      "     |      * 'gain': the average gain across all splits the feature is used in.\n",
      "     |      * 'cover': the average coverage across all splits the feature is used in.\n",
      "     |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      "     |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file.\n",
      "     |      importance_type: str, default 'weight'\n",
      "     |          One of the importance types defined above.\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature, fmap='', bins=None, as_pandas=True)\n",
      "     |      Get split value histogram of a feature\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature: str\n",
      "     |          The name of the feature.\n",
      "     |      fmap: str (optional)\n",
      "     |          The name of feature map file.\n",
      "     |      bin: int, default None\n",
      "     |          The maximum number of bins.\n",
      "     |          Number of bins equals number of unique split values n_unique,\n",
      "     |          if bins == None or bins > n_unique.\n",
      "     |      as_pandas: bool, default True\n",
      "     |          Return pd.DataFrame when pandas is installed.\n",
      "     |          If False or pandas is not installed, return numpy ndarray.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a histogram of used splitting values for the specified feature\n",
      "     |      either as numpy array or pandas DataFrame.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be loaded.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  load_rabit_checkpoint(self)\n",
      "     |      Initialize the model by load from rabit checkpoint.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      version: integer\n",
      "     |          The version number of the model.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False, pred_interactions=False, validate_features=True)\n",
      "     |      Predict with data.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``bst.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      pred_leaf : bool\n",
      "     |          When this option is on, the output will be a matrix of (nsample, ntrees)\n",
      "     |          with each record indicating the predicted leaf index of each sample in each tree.\n",
      "     |          Note that the leaf index of a tree is unique per tree, so you may find leaf 1\n",
      "     |          in both tree 1 and tree 0.\n",
      "     |      \n",
      "     |      pred_contribs : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1)\n",
      "     |          with each record indicating the feature contributions (SHAP values) for that\n",
      "     |          prediction. The sum of all feature contributions is equal to the raw untransformed\n",
      "     |          margin value of the prediction. Note the final column is the bias term.\n",
      "     |      \n",
      "     |      approx_contribs : bool\n",
      "     |          Approximate the contributions of each feature\n",
      "     |      \n",
      "     |      pred_interactions : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)\n",
      "     |          indicating the SHAP interaction values for each pair of features. The sum of each\n",
      "     |          row (or column) of the interaction values equals the corresponding SHAP value (from\n",
      "     |          pred_contribs), and the sum of the entire matrix equals the raw untransformed margin\n",
      "     |          value of the prediction. Note the last row and column correspond to the bias term.\n",
      "     |      \n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be saved.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  save_rabit_checkpoint(self)\n",
      "     |      Save the current booster to rabit checkpoint.\n",
      "     |  \n",
      "     |  save_raw(self)\n",
      "     |      Save the model to a in memory buffer representation\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a in memory buffer representation of the model\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs)\n",
      "     |      Set the attribute of the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
      "     |  \n",
      "     |  set_param(self, params, value=None)\n",
      "     |      Set parameters into the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: dict/list/str\n",
      "     |         list of key,value pairs, dict of key to value or simply str key\n",
      "     |      value: optional\n",
      "     |         value of the specified parameter, when params is str key\n",
      "     |  \n",
      "     |  update(self, dtrain, iteration, fobj=None)\n",
      "     |      Update for one iteration, with objective function calculated internally.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          Training data.\n",
      "     |      iteration : int\n",
      "     |          Current iteration number.\n",
      "     |      fobj : function\n",
      "     |          Customized objective function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  feature_names = None\n",
      "    \n",
      "    class DMatrix(builtins.object)\n",
      "     |  Data Matrix used in XGBoost.\n",
      "     |  \n",
      "     |  DMatrix is a internal data structure that used by XGBoost\n",
      "     |  which is optimized for both memory efficiency and training speed.\n",
      "     |  You can construct DMatrix from numpy.arrays\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string/numpy array/scipy.sparse/pd.DataFrame/DataTable\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string type, it represents the path libsvm format txt file,\n",
      "     |          or binary file that xgboost can read from.\n",
      "     |      label : list or numpy 1-D array, optional\n",
      "     |          Label of the training data.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the data which needs to be present as a missing value. If\n",
      "     |          None, defaults to np.nan.\n",
      "     |      weight : list or numpy 1-D array , optional\n",
      "     |          Weight for each instance.\n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types : list, optional\n",
      "     |          Set types for features.\n",
      "     |      nthread : integer, optional\n",
      "     |          Number of threads to use for loading data from numpy array. If -1,\n",
      "     |          uses maximum threads available on the system.\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of unsigned integer information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of\n",
      "     |      existing model to be base_margin\n",
      "     |      However, remember margin is needed, instead of transformed prediction\n",
      "     |      e.g. for logistic regression: need to put in value before logistic transformation\n",
      "     |      see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_label_npy2d(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |          from numpy 2D array\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |  \n",
      "     |  set_weight_npy2d(self, weight)\n",
      "     |      Set weight of each instance\n",
      "     |          for numpy 2D array\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point in numpy 2D array\n",
      "     |  \n",
      "     |  slice(self, rindex)\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex : list\n",
      "     |          List of indices to be selected.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res : DMatrix\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      "     |      Predict the probability of each `data` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one thread.\n",
      "     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |          of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRanker(XGBModel)\n",
      "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string\n",
      "     |      Specify the learning task and the corresponding learning objective.\n",
      "     |      Only \"rank:pairwise\" is supported currently.\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict\n",
      "     |      simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function is currently not supported by XGBRanker.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  Group information is required for ranking tasks.\n",
      "     |  \n",
      "     |  Before fitting the model, your data need to be sorted by group. When\n",
      "     |  fitting the model, you need to provide an additional array that\n",
      "     |  contains the size of each group.\n",
      "     |  \n",
      "     |  For example, if your original data look like:\n",
      "     |  \n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   qid |   label   |   features    |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_1         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   1       |   x_2         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_3         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   0       |   x_4         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_5         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_6         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_7         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  \n",
      "     |  then your group array should be ``[3, 4]``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRanker\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='rank:pairwise', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, group, sample_weight=None, eval_set=None, sample_weight_eval_set=None, eval_group=None, eval_metric=None, early_stopping_rounds=None, verbose=False, xgb_model=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      group : array_like\n",
      "     |          group size of training data\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_group : list of arrays, optional\n",
      "     |          A list that contains the group size corresponds to each\n",
      "     |          (X, y) pair in eval_set\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each split, in each level.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "        Cross-validation with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round : int\n",
      "            Number of boosting iterations.\n",
      "        nfold : int\n",
      "            Number of folds in CV.\n",
      "        stratified : bool\n",
      "            Perform stratified sampling.\n",
      "        folds : a KFold or StratifiedKFold instance or list of fold indices\n",
      "            Sklearn KFolds or StratifiedKFolds object.\n",
      "            Alternatively may explicitly pass sample indices for each fold.\n",
      "            For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
      "            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
      "            as the training samples for the ``n`` th fold and ``out`` is a list of\n",
      "            indices to be used as the testing samples for the ``n`` th fold.\n",
      "        metrics : string or list of strings\n",
      "            Evaluation metrics to be watched in CV.\n",
      "        obj : function\n",
      "            Custom objective function.\n",
      "        feval : function\n",
      "            Custom evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. CV error needs to decrease at least\n",
      "            every <early_stopping_rounds> round(s) to continue.\n",
      "            Last entry in evaluation history is the one from best iteration.\n",
      "        fpreproc : function\n",
      "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "            transformed versions of those.\n",
      "        as_pandas : bool, default True\n",
      "            Return pd.DataFrame when pandas is installed.\n",
      "            If False or pandas is not installed, return np.ndarray\n",
      "        verbose_eval : bool, int, or None, default None\n",
      "            Whether to display the progress. If None, progress will be displayed\n",
      "            when np.ndarray is returned. If True, progress will be displayed at\n",
      "            boosting stage. If an integer is given, progress will be displayed\n",
      "            at every given `verbose_eval` boosting stage.\n",
      "        show_stdv : bool, default True\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected, and always contains std.\n",
      "        seed : int\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        shuffle : bool\n",
      "            Shuffle data before creating folds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        evaluation history : list(string)\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "        Plot importance based on fitted trees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel or dict\n",
      "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "        importance_type : str, default \"weight\"\n",
      "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "        \n",
      "            * \"weight\" is the number of times a feature appears in a tree\n",
      "            * \"gain\" is the average gain of splits which use the feature\n",
      "            * \"cover\" is the average coverage of splits which use the feature\n",
      "              where coverage is defined as the number of samples affected by the split\n",
      "        max_num_features : int, default None\n",
      "            Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "        height : float, default 0.2\n",
      "            Bar height, passed to ax.barh()\n",
      "        xlim : tuple, default None\n",
      "            Tuple passed to axes.xlim()\n",
      "        ylim : tuple, default None\n",
      "            Tuple passed to axes.ylim()\n",
      "        title : str, default \"Feature importance\"\n",
      "            Axes title. To disable, pass None.\n",
      "        xlabel : str, default \"F score\"\n",
      "            X axis title label. To disable, pass None.\n",
      "        ylabel : str, default \"Features\"\n",
      "            Y axis title label. To disable, pass None.\n",
      "        show_values : bool, default True\n",
      "            Show values on plot. To disable, pass False.\n",
      "        kwargs :\n",
      "            Other keywords passed to ax.barh()\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    plot_tree(booster, fmap='', num_trees=0, rankdir='UT', ax=None, **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        kwargs :\n",
      "            Other keywords passed to to_graphviz\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    to_graphviz(booster, fmap='', num_trees=0, rankdir='UT', yes_color='#0000FF', no_color='#FF0000', **kwargs)\n",
      "        Convert specified tree to graphviz instance. IPython can automatically plot the\n",
      "        returned graphiz instance. Otherwise, you should call .render() method\n",
      "        of the returned graphiz instance.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        yes_color : str, default '#0000FF'\n",
      "            Edge color when meets the node condition.\n",
      "        no_color : str, default '#FF0000'\n",
      "            Edge color when doesn't meet the node condition.\n",
      "        kwargs :\n",
      "            Other keywords passed to graphviz graph_attr\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "        Train a booster with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round: int\n",
      "            Number of boosting iterations.\n",
      "        evals: list of pairs (DMatrix, string)\n",
      "            List of items to be evaluated during training, this allows user to watch\n",
      "            performance on the validation set.\n",
      "        obj : function\n",
      "            Customized objective function.\n",
      "        feval : function\n",
      "            Customized evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Validation error needs to decrease at least\n",
      "            every **early_stopping_rounds** round(s) to continue training.\n",
      "            Requires at least one item in **evals**.\n",
      "            If there's more than one, will use the last.\n",
      "            Returns the model from the last iteration (not the best one).\n",
      "            If early stopping occurs, the model will have three additional fields:\n",
      "            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "            (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "        evals_result: dict\n",
      "            This dictionary stores the evaluation results of all the items in watchlist.\n",
      "        \n",
      "            Example: with a watchlist containing\n",
      "            ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "            a parameter containing ``('eval_metric': 'logloss')``,\n",
      "            the **evals_result** returns\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "                 'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "        \n",
      "        verbose_eval : bool or int\n",
      "            Requires at least one item in **evals**.\n",
      "            If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "            printed at each boosting stage.\n",
      "            If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "            is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "            / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "            is printed every 4 boosting stages, instead of every boosting stage.\n",
      "        learning_rates: list or function (deprecated - use callback API instead)\n",
      "            List of learning rate for each boosting round\n",
      "            or a customized function that calculates eta in terms of\n",
      "            current number of round and the total number of boosting round (e.g. yields\n",
      "            learning rate decay)\n",
      "        xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "            Xgb model to be loaded before training (allows training continuation).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Booster : a trained booster model\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DMatrix', 'Booster', 'train', 'cv', 'XGBModel', 'XGBClassi...\n",
      "\n",
      "VERSION\n",
      "    0.81\n",
      "\n",
      "FILE\n",
      "    c:\\users\\trishul\\anaconda3\\lib\\site-packages\\xgboost\\__init__.py\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
